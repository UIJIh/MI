{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel, PeftConfig\n",
    "from tqdm import tqdm\n",
    "import csv\n",
    "\n",
    "def setup_model_and_tokenizer(model_path):\n",
    "    config = PeftConfig.from_pretrained(model_path)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n",
    "    special_tokens = {\"pad_token\": \"<pad>\", \"eos_token\": \"<eos>\", \"bos_token\": \"<bos>\"}\n",
    "    tokenizer.add_special_tokens(special_tokens)\n",
    "    \n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\n",
    "        config.base_model_name_or_path,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        load_in_4bit=True,\n",
    "    )\n",
    "    base_model.resize_token_embeddings(len(tokenizer))    \n",
    "    model = PeftModel.from_pretrained(base_model, model_path)\n",
    "    \n",
    "    return model, tokenizer\n",
    "\n",
    "def generate_response(prompt, model, tokenizer, max_new_tokens=100):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=200)\n",
    "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            num_return_sequences=1,\n",
    "            temperature=0.7,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    response = response[len(prompt):].strip()\n",
    "    return response\n",
    "\n",
    "def process_csv(input_path, output_path, system_prompt, sentence_column, model, tokenizer):\n",
    "    df = pd.read_csv(input_path)\n",
    "    #print(\"Columns in CSV:\", df.columns)  \n",
    "\n",
    "    with open(output_path, 'w', newline='', encoding='utf-8') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([sentence_column, 'detected_idiom'])\n",
    "        \n",
    "        for sentence in tqdm(df[sentence_column], desc=f\"Processing sentences in {sentence_column}\"):\n",
    "            full_prompt = system_prompt + f\"\\nSentence: \\\"{sentence}\\\"\\nOutput:\"\n",
    "            detected_idiom = generate_response(full_prompt, model, tokenizer)\n",
    "            print(f\"{sentence} : {detected_idiom}\")\n",
    "            writer.writerow([sentence, detected_idiom])\n",
    "\n",
    "def main():\n",
    "    model_path = \"/data/uijih/detection/saveded_instruct-70-detection-new\"\n",
    "    input_csv_path = '/data/uijih/uiji_seed.csv'\n",
    "    output_csv_path_en = \"./70b-inst-en-none.csv\"\n",
    "    output_csv_path_kr = \"./70b-inst-kr-none.csv\"\n",
    "    \n",
    "    # 사용자 요청 원문 프롬프트\n",
    "    system_prompt_en = \"\"\"The task is to detect any idiom present in the given sentence and return it exactly as it appears in the sentence. If there is no idiom, respond with \\\"None.\\\" and provide no additional text or explanation. Follow the examples below:\n",
    "\n",
    "    Example 1:\n",
    "    Sentence: \"She plans to travel the world before she kicks the bucket.\"\n",
    "    Output: \"kicks the bucket\"\n",
    "\n",
    "    Example 2:\n",
    "    Sentence: \"I think I ate too much dinner, and now my stomach hurts.\"\n",
    "    Output: \"None\"\n",
    "\n",
    "    Now, process the following sentence.\n",
    "    \"\"\"\n",
    "    \n",
    "    system_prompt_kr = \"\"\"The task is to detect any idiom present in the given sentence and return it exactly as it appears in the sentence. If there is no idiom, respond with \\\"None.\\\" and provide no additional text or explanation. Follow the examples below:\n",
    "\n",
    "    Example 1:\n",
    "    Sentence: \"가족을 먹여살리고자 밤낮 없이 일하다 불의의 사고로 한 줌 재가 되어버린 그가 남긴 자산을 그리 많지 않았다.\"\n",
    "    Output: \"한 줌 재가 되어버린\"\n",
    "\n",
    "    Example 2:\n",
    "    Sentence: \"저녁을 너무 많이 먹어서인지 배가 아프다.\"\n",
    "    Output: \"None\"\n",
    "\n",
    "    Now, process the following sentence.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Loading model and tokenizer...\")  \n",
    "    model, tokenizer = setup_model_and_tokenizer(model_path)\n",
    "    \n",
    "    print(\"Processing with English system prompt...\")\n",
    "    process_csv(input_csv_path, output_csv_path_en, system_prompt_en, 'Sentence', model, tokenizer)\n",
    "    \n",
    "    print(\"Processing with Korean system prompt...\")\n",
    "    process_csv(input_csv_path, output_csv_path_kr, system_prompt_kr, 'KRSentence', model, tokenizer)\n",
    "    \n",
    "    print(\"Inference completed successfully!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "# 1. 추론 함수 정의\n",
    "def inference(model_path, tokenizer, input_data, system_prompts_en, labels=None, language='en'):\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_path, device_map=\"auto\")\n",
    "    model.eval()\n",
    "\n",
    "    # 영어 프롬프트 템플릿\n",
    "    prompt_template = \"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "{system_prompt}<|eot_id|>\n",
    "<|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "{user_input}<|eot_id|>\n",
    "<|start_header_id|>assistant<|end_header_id>\n",
    "\"\"\"\n",
    "\n",
    "    results = []\n",
    "    for idx, sentence in enumerate(input_data):\n",
    "        # 영어 또는 한국어 프롬프트 사용\n",
    "        system_prompt = random.choice(system_prompts_en)\n",
    "        \n",
    "        if language == 'en':\n",
    "            user_input = f'Is there an idiom in the sentence \"{sentence}\"? If yes, return only the detected idiom. Generate just the idiom in its original form without any additional explanation or text. If there is no idiom, answer \"none\".'\n",
    "        # else:  # 한국어 문장에 대해 처리\n",
    "        #     user_input = f'\"{sentence}\"라는 문장에서 관용구가 있니? 있으면 관용구만 알려줘. 없으면 \"없음\"이라고 대답해.'\n",
    "\n",
    "        # 프롬프트 템플릿에 시스템 프롬프트와 사용자 입력 삽입\n",
    "        input_prompt = prompt_template.format(\n",
    "            system_prompt=system_prompt,\n",
    "            user_input=user_input\n",
    "        )\n",
    "\n",
    "        # 토크나이즈\n",
    "        inputs = tokenizer(input_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "        # 모델에 입력\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_length=256,\n",
    "                do_sample=False,\n",
    "                num_beams=5, \n",
    "                early_stopping=True, \n",
    "                repetition_penalty=1.1\n",
    "            )\n",
    "\n",
    "        # 토큰 디코딩 및 정리된 결과만 추가\n",
    "        decoded_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        cleaned_output = decoded_output.split('assistant<|end_header_id>')[-1].strip()  # 불필요한 부분 제거\n",
    "        # 결과만 추가\n",
    "        results.append(cleaned_output)\n",
    "\n",
    "        # 바로 출력\n",
    "        if labels:\n",
    "            print(f\"\\nSentence: {sentence}\")\n",
    "            print(f\"Label: {labels[idx]}\")\n",
    "            print(f\"Prediction: {cleaned_output}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# 2. 추론 및 결과 저장 함수\n",
    "def run_inference_and_save_results(input_csv_path, output_csv_path, model_path, system_prompts_en, language='en'):\n",
    "    # 데이터 로드\n",
    "    data = pd.read_csv(input_csv_path)\n",
    "    \n",
    "    # 문장과 레이블 분리 (영어/한국어에 따라 분리)\n",
    "    if language == 'en':\n",
    "        sentences = data['Sentence'].tolist()  # 추론할 문장 리스트\n",
    "        labels = data['Idiom'].tolist()  # 실제 레이블 (정답)\n",
    "    else:\n",
    "        sentences = data['KRSentence'].tolist()  # 한국어 문장\n",
    "        labels = data['KRIdiom'].tolist()  # 실제 레이블 (한국어 관용구)\n",
    "\n",
    "    # 모델 및 토크나이저 로드\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "    # 추론 실행\n",
    "    print(f\"Running inference with model: {model_path} ({language})\")\n",
    "    model_predictions = inference(model_path, tokenizer, sentences, system_prompts_en, labels, 'en') # en으로 input language 통일 (학습때도 그랬음)\n",
    "\n",
    "    # 결과 데이터프레임 생성 및 저장\n",
    "    result_df = pd.DataFrame({\n",
    "        'Sentence': sentences,\n",
    "        'Label': labels,  \n",
    "        'Model_Prediction': model_predictions  \n",
    "    })\n",
    "    result_df.to_csv(output_csv_path, index=False)\n",
    "    print(f\"\\nResults saved to {output_csv_path}\")\n",
    "\n",
    "    return result_df\n",
    "\n",
    "# 3. 실행\n",
    "if __name__ == \"__main__\":\n",
    "    model_path = \"/data/uijih/detection/saveded_instruct-full-detection-1\"\n",
    "    #model_path = \"./saveded_instruct-70-detection-l\"\n",
    "    #model_path = \"meta-llama/Meta-Llama-3.1-70B-Instruct\"\n",
    "    #input_csv_path = \"/data/uijih/previous/Seed50_for_Parallel_Dataset_ENKR_idiomKB_0.8_example.csv\"\n",
    "    input_csv_path = \"/data/uijih/uiji_seed.csv\"\n",
    "    output_csv_path_en = \"./8b-inst-full-en.csv\"  \n",
    "    output_csv_path_kr = \"./8b-inst-full-kr.csv\"  \n",
    "\n",
    "    # 학습 때 사용했던 영어 시스템 프롬프트만 사용 (일단 하나만)\n",
    "    system_prompts_en = [\n",
    "        \"Detect if there is an idiom in the following sentence. If there is, return only the detected idiom in its original form. If there are no idioms, answer 'none'.\"\n",
    "        # \"Check if the following sentence contains any idioms. If so, return the idiom only. If none, respond with 'none'.\",\n",
    "        # \"Look for an idiom in the following sentence and return the idiom only if found. Otherwise, return 'none'.\"\n",
    "    ]\n",
    "\n",
    "    result_df_en = run_inference_and_save_results(input_csv_path, output_csv_path_en, model_path, system_prompts_en, language='en')    \n",
    "    result_df_kr = run_inference_and_save_results(input_csv_path, output_csv_path_kr, model_path, system_prompts_en, language='kr')\n",
    "\n",
    "    # Display the DataFrames for visualization in Jupyter\n",
    "    # import ace_tools as tools\n",
    "    # tools.display_dataframe_to_user(name=\"English Inference Results\", dataframe=result_df_en)\n",
    "    # tools.display_dataframe_to_user(name=\"Korean Inference Results\", dataframe=result_df_kr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "# 1. 추론 함수 정의\n",
    "def inference(model_path, tokenizer, input_data, system_prompts_en, labels=None, language='en'):\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_path, device_map=\"auto\")\n",
    "    model.eval()\n",
    "\n",
    "    # 영어와 한국어 프롬프트 템플릿 정의\n",
    "    prompt_template_en = \"\"\"\n",
    "    Detect if there is an idiom in the following sentence. If there is, return only the detected idiom. Generate just the idiom without any additional explanation or text. If there are no idioms, answer 'none'.\n",
    "\n",
    "    # Sentence:\n",
    "    {user_input}\n",
    "    \n",
    "    # Detected Idiom:\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    for idx, sentence in enumerate(input_data):\n",
    "        if language == 'en':\n",
    "            input_prompt = prompt_template_en.format(user_input=sentence)  # 영어 프롬프트\n",
    "        # else:\n",
    "        #     user_input = f'{sentence}'\n",
    "        #     input_prompt = prompt_template_kr.format(user_input=user_input)  # 한국어 -> 영어 번역 프롬프트\n",
    "\n",
    "        # 토크나이즈\n",
    "        inputs = tokenizer(input_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "        # 모델에 입력\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_length=256,\n",
    "                do_sample=False,\n",
    "                num_beams=5, \n",
    "                early_stopping=True, \n",
    "                repetition_penalty=1.1\n",
    "            )\n",
    "\n",
    "        # 토큰 디코딩 및 정리된 결과만 추가\n",
    "        decoded_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        cleaned_output = decoded_output.split('# Detected Idiom:')[-1].strip()  # 불필요한 부분 제거\n",
    "        # 결과만 추가\n",
    "        results.append(cleaned_output)\n",
    "\n",
    "        # 바로 출력\n",
    "        if labels:\n",
    "            print(f\"\\nSentence: {sentence}\")\n",
    "            print(f\"Label: {labels[idx]}\")\n",
    "            print(f\"Prediction: {cleaned_output}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# 2. 추론 및 결과 저장 함수\n",
    "def run_inference_and_save_results(input_csv_path, output_csv_path, model_path, system_prompts_en, language='en'):\n",
    "    # 데이터 로드\n",
    "    data = pd.read_csv(input_csv_path)\n",
    "    \n",
    "    # 문장과 레이블 분리 (영어/한국어에 따라 분리)\n",
    "    if language == 'en':\n",
    "        sentences = data['Sentence'].tolist()  # 추론할 문장 리스트\n",
    "        labels = data['Idiom'].tolist()  # 실제 레이블 (정답)\n",
    "    else:\n",
    "        sentences = data['KR_Sentence'].tolist()  # 한국어 문장\n",
    "        labels = data['KR_Idiom'].tolist()  # 실제 레이블 (한국어 관용구)\n",
    "\n",
    "    # 모델 및 토크나이저 로드\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "    # 추론 실행\n",
    "    print(f\"Running inference with model: {model_path} ({language})\")\n",
    "    model_predictions = inference(model_path, tokenizer, sentences, system_prompts_en, labels, 'en')  # 'kr' 또는 'en'에 따라 추론\n",
    "\n",
    "    # 결과 데이터프레임 생성 및 저장\n",
    "    result_df = pd.DataFrame({\n",
    "        'Sentence': sentences,\n",
    "        'Label': labels,  \n",
    "        'Model_Prediction': model_predictions  \n",
    "    })\n",
    "    result_df.to_csv(output_csv_path, index=False)\n",
    "    print(f\"\\nResults saved to {output_csv_path}\")\n",
    "\n",
    "    return result_df\n",
    "\n",
    "# 3. 실행\n",
    "if __name__ == \"__main__\": \n",
    "    model_path = \"meta-llama/Meta-Llama-3.1-8B\"\n",
    "    input_csv_path = \"/data/uijih/previous/Seed50_for_Parallel_Dataset_ENKR_idiomKB_0.8_example.csv\"\n",
    "    output_csv_path_en = \"./8b-base-en.csv\"  \n",
    "    output_csv_path_kr = \"./8b-base-kr.csv\"  \n",
    "\n",
    "    # 영어 시스템 프롬프트\n",
    "    system_prompts_en = [\n",
    "        \"Detect if there is an idiom in the following sentence. If there is, return only the detected idiom. If there are no idioms, answer 'none'.\",\n",
    "    ]\n",
    "\n",
    "    # 영어 및 한국어 추론 실행\n",
    "    result_df_en = run_inference_and_save_results(input_csv_path, output_csv_path_en, model_path, system_prompts_en, language='en')    \n",
    "    result_df_kr = run_inference_and_save_results(input_csv_path, output_csv_path_kr, model_path, system_prompts_en, language='kr')\n",
    "\n",
    "    # Display the DataFrames for visualization in Jupyter\n",
    "    import ace_tools as tools\n",
    "    tools.display_dataframe_to_user(name=\"English Inference Results\", dataframe=result_df_en)\n",
    "    tools.display_dataframe_to_user(name=\"Korean Inference Results\", dataframe=result_df_kr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============= 샘플 평가 =============\n",
      "입력 문장: 그는 상을 받아 잔치날에 큰상 받는 기분이었다.\n",
      "Ground Truth: 잔치날에 큰상 받는 기분\n",
      "모델 예측: 큰 상\n",
      "\n",
      "============= 샘플 평가 =============\n",
      "입력 문장: 그는 고슴도치 외 따 지듯 빚을 졌다.\n",
      "Ground Truth: 고슴도치 외 따 지듯\n",
      "모델 예측: 고슴도치 외 따 지듯\n",
      "\n",
      "============= 샘플 평가 =============\n",
      "입력 문장: 그 집은 아름다운 정원이 있어요.\n",
      "Ground Truth: None\n",
      "모델 예측: None\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch, json, re\n",
    "from datasets import Dataset\n",
    "\n",
    "\n",
    "# 8B 모델 로드\n",
    "# model_id = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "# tokenizer.add_special_tokens({\"pad_token\": \"<pad>\", \"eos_token\": \"<eos>\", \"bos_token\": \"<bos>\"})\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\").eval()\n",
    "# # 토크나이저에 새로 추가된 토큰 적용\n",
    "# model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# 데이터셋 로드 함수\n",
    "def load_dataset(jsonl_path, tokenizer, num_samples=1):\n",
    "    with open(jsonl_path, 'r') as f:\n",
    "        raw_data = [json.loads(line) for line in f]\n",
    "\n",
    "    data = {'text': []}\n",
    "    for item in raw_data[:num_samples]:  # 샘플 개수 제한\n",
    "        conversation = []\n",
    "        for entry in item[\"conversations\"]:\n",
    "            conversation.append({\"role\": \"system\", \"content\": \"You are an expert in detecting idioms. Identify only the idiom exactly as it appears in the given sentence. Do not provide additional explanations or context interpretation. If there is no idiom, respond with 'None.'\"})\n",
    "            conversation.append({'role': 'user', 'content': entry['user']})\n",
    "            conversation.append({'role': 'assistant', 'content': entry['assistant']})\n",
    "        \n",
    "        templated = tokenizer.apply_chat_template(conversation, tokenize=False, padding=True, max_length=400, truncation=True)\n",
    "        data['text'].append(templated)\n",
    "\n",
    "    return Dataset.from_dict(data)\n",
    "\n",
    "# 테스트용 데이터셋 로드\n",
    "test_dataset = load_dataset(\"test_idioms_detection_dataset.jsonl\", tokenizer, num_samples=3)\n",
    "\n",
    "# 평가 콜백 클래스\n",
    "class AdvancedIdiomEvalCallback:\n",
    "    def __init__(self, model, eval_dataset, tokenizer, num_examples=3):\n",
    "        self.model = model\n",
    "        self.eval_dataset = eval_dataset\n",
    "        self.tokenizer = tokenizer\n",
    "        self.num_examples = num_examples\n",
    "\n",
    "    def extract_user_input(self, full_text):\n",
    "        \"\"\"\n",
    "        Extracts the 'user' content from the text based on markers.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            user_start = \"<|start_header_id|>user<|end_header_id|>\"\n",
    "            user_end = \"<|eot_id|>\"\n",
    "            user_content = full_text.split(user_start)[-1].split(user_end)[0].strip()\n",
    "            return user_content\n",
    "        except IndexError:\n",
    "            return full_text.strip()\n",
    "\n",
    "    def extract_ground_truth(self, full_text):\n",
    "        \"\"\"\n",
    "        Extracts the 'assistant' content from the text based on markers.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            assistant_start = \"<|start_header_id|>assistant<|end_header_id|>\"\n",
    "            assistant_end = \"<|eot_id|>\"\n",
    "            assistant_content = full_text.split(assistant_start)[-1].split(assistant_end)[0].strip()\n",
    "            return assistant_content\n",
    "        except IndexError:\n",
    "            return \"None\"\n",
    "\n",
    "    def evaluate(self):\n",
    "        for i in range(min(self.num_examples, len(self.eval_dataset))):\n",
    "            sample_data = self.eval_dataset[i]\n",
    "            full_text = sample_data[\"text\"]\n",
    "            user_input = self.extract_user_input(full_text)\n",
    "            ground_truth = self.extract_ground_truth(full_text)\n",
    "\n",
    "            eval_conversation = [\n",
    "                {\"role\": \"system\", \"content\": \"You are an expert in detecting idioms. Identify only the idiom exactly as it appears in the given sentence. Do not provide additional explanations or context interpretation. If there is no idiom, respond with 'None.'\"},\n",
    "                {\"role\": \"user\", \"content\": user_input}\n",
    "            ]\n",
    "\n",
    "            # 입력 텍스트를 토큰화\n",
    "            eval_text = self.tokenizer.apply_chat_template(eval_conversation, tokenize=False)\n",
    "            inputs = self.tokenizer(eval_text, return_tensors=\"pt\", padding=True, truncation=True).to(self.model.device)\n",
    "            print(\"\\n============= 샘플 평가 =============\")\n",
    "            print(f\"입력 문장: {eval_text}\") # userinput으로 해야지 실제 문장\n",
    "            # 모델 출력 생성\n",
    "            outputs = self.model.generate(\n",
    "                inputs[\"input_ids\"],\n",
    "                max_length=256,\n",
    "                num_return_sequences=1,\n",
    "                do_sample=False,\n",
    "                pad_token_id=self.tokenizer.pad_token_id\n",
    "            )\n",
    "\n",
    "            # 전체 생성 결과 디코딩\n",
    "            decoded_output = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            #print(f\"Decoded Output: {decoded_output}\")  # 디버깅 출력\n",
    "\n",
    "            # \"assistant\" 이후 텍스트 추출\n",
    "            if \"assistant\" in decoded_output:\n",
    "                detected_idiom = decoded_output.split(\"assistant\")[-1].strip()\n",
    "            else:\n",
    "                detected_idiom = decoded_output.strip()\n",
    "            \n",
    "            print(f\"Ground Truth: {ground_truth}\")\n",
    "            print(f\"모델 예측: {detected_idiom}\")\n",
    "\n",
    "\n",
    "\n",
    "# 콜백 평가 실행\n",
    "callback = AdvancedIdiomEvalCallback(model=model, eval_dataset=test_dataset, tokenizer=tokenizer)\n",
    "callback.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3.11.3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
