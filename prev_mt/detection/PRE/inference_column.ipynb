{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "kr = pd.read_csv(\"/data/uijih/detection/8b-inst-full-kr.csv\")\n",
    "en = pd.read_csv(\"/data/uijih/detection/8b-inst-full-en.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kr = pd.DataFrame(kr)\n",
    "en = pd.DataFrame(en)\n",
    "\n",
    "#count_k = kr['ans'].value_counts().get('o', 0)\n",
    "count_e = en['ans'].value_counts().get('o', 0)\n",
    "print(count_e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(count_k, count_e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_k = kr['ans'].value_counts().get('x', 0)\n",
    "count_e = en['ans'].value_counts().get('x', 0)\n",
    "print(count_k, count_e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_k = kr['ans'].value_counts().get('m', 0)\n",
    "count_e = en['ans'].value_counts().get('m', 0)\n",
    "print(count_k, count_e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install thefuzz[speedup]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from difflib import SequenceMatcher \n",
    "\n",
    "dataset = pd.read_csv('/data/uijih/previous/Seed50_for_Parallel_Dataset_ENKR_idiomKB_0.8_example.csv')\n",
    "results = pd.read_csv('/data/uijih/detection/70b-inst-kr-2.csv')\n",
    "\n",
    "# 0~1이고 높을수록 유사\n",
    "SIMILARITY_THRESHOLD = 0.6\n",
    "matched_results = []\n",
    "\n",
    "for prediction in results['Model_Prediction']:\n",
    "    print(prediction)\n",
    "    print(\"=\"*20)\n",
    "    highest_similarity = 0\n",
    "    best_match = None\n",
    "\n",
    "    # 전처리\n",
    "    #cleaned_prediction = prediction.replace('<|end_header_id>', '').strip()  \n",
    "    # kr\n",
    "    prediction = str(prediction).strip()  \n",
    "    for idiom in dataset['KR_Idiom']:\n",
    "        #prediction_lower = cleaned_prediction.lower()\n",
    "        #idiom_lower = idiom.lower()        \n",
    "        # 유사도 계산\n",
    "        #similarity = SequenceMatcher(None, prediction_lower, idiom_lower).ratio()\n",
    "        # kr\n",
    "        idiom = str(idiom).strip()  \n",
    "        print(idiom, prediction)\n",
    "        similarity = SequenceMatcher(None, str(prediction), idiom).ratio()\n",
    "        if similarity > highest_similarity:\n",
    "            highest_similarity = similarity\n",
    "            best_match = idiom\n",
    "\n",
    "    if highest_similarity >= SIMILARITY_THRESHOLD:\n",
    "        matched_results.append({\n",
    "            #'Model_Prediction': cleaned_prediction,\n",
    "            'Model_Prediction': prediction,\n",
    "            'Matched_Idiom': best_match,\n",
    "            'Similarity_Score': highest_similarity\n",
    "        })\n",
    "    else:\n",
    "        matched_results.append({\n",
    "            #'Model_Prediction': cleaned_prediction,\n",
    "            'Model_Prediction': prediction,\n",
    "            'Matched_Idiom': None,\n",
    "            'Similarity_Score': None\n",
    "        })\n",
    "\n",
    "matched_df = pd.DataFrame(matched_results)\n",
    "matched_df.to_csv('matched_results.csv', index=False)\n",
    "\n",
    "print(matched_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(matched_df['Matched_Idiom']==None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8b 2 inference한거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "# 1. 추론 함수 정의\n",
    "def inference(model_path, tokenizer, input_data, system_prompts_en, labels=None, language='en'):\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_path, device_map=\"auto\")\n",
    "    model.eval()\n",
    "\n",
    "    # 영어 프롬프트 템플릿\n",
    "    prompt_template = \"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "{system_prompt}<|eot_id|>\n",
    "<|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "{user_input}<|eot_id|>\n",
    "<|start_header_id|>assistant<|end_header_id>\n",
    "\"\"\"\n",
    "\n",
    "    results = []\n",
    "    for idx, sentence in enumerate(input_data):\n",
    "        # 영어 또는 한국어 프롬프트 사용\n",
    "        system_prompt = random.choice(system_prompts_en)\n",
    "        \n",
    "        if language == 'en':\n",
    "            user_input = f'Is there an idiom in the sentence \"{sentence}\"? If yes, return only the detected idiom. Generate just the idiom in its original form without any additional explanation or text. If there is no idiom, answer \"none\".'\n",
    "        # else:  # 한국어 문장에 대해 처리\n",
    "        #     user_input = f'\"{sentence}\"라는 문장에서 관용구가 있니? 있으면 관용구만 알려줘. 없으면 \"없음\"이라고 대답해.'\n",
    "\n",
    "        # 프롬프트 템플릿에 시스템 프롬프트와 사용자 입력 삽입\n",
    "        input_prompt = prompt_template.format(\n",
    "            system_prompt=system_prompt,\n",
    "            user_input=user_input\n",
    "        )\n",
    "\n",
    "        # 토크나이즈\n",
    "        inputs = tokenizer(input_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "        # 모델에 입력\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_length=256,\n",
    "                do_sample=False,\n",
    "                num_beams=5, \n",
    "                early_stopping=True, \n",
    "                repetition_penalty=1.1\n",
    "            )\n",
    "\n",
    "        # 토큰 디코딩 및 정리된 결과만 추가\n",
    "        decoded_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        cleaned_output = decoded_output.split('assistant<|end_header_id>')[-1].strip()  # 불필요한 부분 제거\n",
    "        # 결과만 추가\n",
    "        results.append(cleaned_output)\n",
    "\n",
    "        # 바로 출력\n",
    "        if labels:\n",
    "            print(f\"\\nSentence: {sentence}\")\n",
    "            print(f\"Label: {labels[idx]}\")\n",
    "            print(f\"Prediction: {cleaned_output}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# 2. 추론 및 결과 저장 함수\n",
    "def run_inference_and_save_results(input_csv_path, output_csv_path, model_path, system_prompts_en, language='en'):\n",
    "    # 데이터 로드\n",
    "    data = pd.read_csv(input_csv_path)\n",
    "    \n",
    "    # 문장과 레이블 분리 (영어/한국어에 따라 분리)\n",
    "    if language == 'en':\n",
    "        sentences = data['Sentence'].tolist()  # 추론할 문장 리스트\n",
    "        labels = data['Idiom'].tolist()  # 실제 레이블 (정답)\n",
    "    else:\n",
    "        sentences = data['KR_Sentence'].tolist()  # 한국어 문장\n",
    "        labels = data['KR_Idiom'].tolist()  # 실제 레이블 (한국어 관용구)\n",
    "\n",
    "    # 모델 및 토크나이저 로드\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "    # 추론 실행\n",
    "    print(f\"Running inference with model: {model_path} ({language})\")\n",
    "    model_predictions = inference(model_path, tokenizer, sentences, system_prompts_en, labels, 'en') # en으로 input language 통일 (학습때도 그랬음)\n",
    "\n",
    "    # 결과 데이터프레임 생성 및 저장\n",
    "    result_df = pd.DataFrame({\n",
    "        'Sentence': sentences,\n",
    "        'Label': labels,  \n",
    "        'Model_Prediction': model_predictions  \n",
    "    })\n",
    "    result_df.to_csv(output_csv_path, index=False)\n",
    "    print(f\"\\nResults saved to {output_csv_path}\")\n",
    "\n",
    "    return result_df\n",
    "\n",
    "# 3. 실행\n",
    "if __name__ == \"__main__\":\n",
    "    model_path = \"/data/uijih/detection/saveded_instruct-full-detection-2\"\n",
    "    #model_path = \"meta-llama/Meta-Llama-3.1-70B-Instruct\"\n",
    "    input_csv_path = \"/data/uijih/previous/Seed50_for_Parallel_Dataset_ENKR_idiomKB_0.8_example.csv\"\n",
    "    output_csv_path_en = \"./8b-inst-full-en-2.csv\"  \n",
    "    output_csv_path_kr = \"./8b-inst-full-kr-2.csv\"  \n",
    "\n",
    "    # 학습 때 사용했던 영어 시스템 프롬프트만 사용 (일단 하나만)\n",
    "    system_prompts_en = [\n",
    "        \"Detect if there is an idiom in the following sentence. If there is, return only the detected idiom in its original form. If there are no idioms, answer 'none'.\"\n",
    "        # \"Check if the following sentence contains any idioms. If so, return the idiom only. If none, respond with 'none'.\",\n",
    "        # \"Look for an idiom in the following sentence and return the idiom only if found. Otherwise, return 'none'.\"\n",
    "    ]\n",
    "\n",
    "    result_df_en = run_inference_and_save_results(input_csv_path, output_csv_path_en, model_path, system_prompts_en, language='en')    \n",
    "    result_df_kr = run_inference_and_save_results(input_csv_path, output_csv_path_kr, model_path, system_prompts_en, language='kr')\n",
    "\n",
    "    # Display the DataFrames for visualization in Jupyter\n",
    "    # import ace_tools as tools\n",
    "    # tools.display_dataframe_to_user(name=\"English Inference Results\", dataframe=result_df_en)\n",
    "    # tools.display_dataframe_to_user(name=\"Korean Inference Results\", dataframe=result_df_kr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# meaning을 너무 잘 맞춰서 이상하길래 위에껄로 한번 translation시켜보자 -> ㄴㄴ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "# 1. 추론 함수 정의\n",
    "def inference(model_path, tokenizer, input_data, system_prompts_en, labels=None, language='en'):\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_path, device_map=\"auto\")\n",
    "    model.eval()\n",
    "\n",
    "    # 영어 프롬프트 템플릿\n",
    "    prompt_template = \"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "{system_prompt}<|eot_id|>\n",
    "<|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "{user_input}<|eot_id|>\n",
    "<|start_header_id|>assistant<|end_header_id>\n",
    "\"\"\"\n",
    "\n",
    "    results = []\n",
    "    for idx, sentence in enumerate(input_data):\n",
    "        # 영어 또는 한국어 프롬프트 사용\n",
    "        system_prompt = random.choice(system_prompts_en)\n",
    "        \n",
    "        if language == 'en':\n",
    "            user_input = f'Translate the sentence \"{sentence}\" into Korean.'\n",
    "        # else:  # 한국어 문장에 대해 처리\n",
    "        #     user_input = f'\"{sentence}\"라는 문장에서 관용구가 있니? 있으면 관용구만 알려줘. 없으면 \"없음\"이라고 대답해.'\n",
    "\n",
    "        # 프롬프트 템플릿에 시스템 프롬프트와 사용자 입력 삽입\n",
    "        input_prompt = prompt_template.format(\n",
    "            system_prompt=system_prompt,\n",
    "            user_input=user_input\n",
    "        )\n",
    "\n",
    "        # 토크나이즈\n",
    "        inputs = tokenizer(input_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "        # 모델에 입력\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_length=256,\n",
    "                do_sample=False,\n",
    "                num_beams=5, \n",
    "                early_stopping=True, \n",
    "                repetition_penalty=1.1\n",
    "            )\n",
    "\n",
    "        # 토큰 디코딩 및 정리된 결과만 추가\n",
    "        decoded_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        cleaned_output = decoded_output.split('assistant<|end_header_id>')[-1].strip()  # 불필요한 부분 제거\n",
    "        # 결과만 추가\n",
    "        results.append(cleaned_output)\n",
    "\n",
    "        # 바로 출력\n",
    "        if labels:\n",
    "            print(f\"\\nSentence: {sentence}\")\n",
    "            print(f\"Label: {labels[idx]}\")\n",
    "            print(f\"Prediction: {cleaned_output}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# 2. 추론 및 결과 저장 함수\n",
    "def run_inference_and_save_results(input_csv_path, output_csv_path, model_path, system_prompts_en, language='en'):\n",
    "    # 데이터 로드\n",
    "    data = pd.read_csv(input_csv_path)\n",
    "    \n",
    "    # 문장과 레이블 분리 (영어/한국어에 따라 분리)\n",
    "    if language == 'en':\n",
    "        sentences = data['Sentence'].tolist()  # 추론할 문장 리스트\n",
    "        labels = data['Idiom'].tolist()  # 실제 레이블 (정답)\n",
    "    else:\n",
    "        sentences = data['KR_Sentence'].tolist()  # 한국어 문장\n",
    "        labels = data['KR_Idiom'].tolist()  # 실제 레이블 (한국어 관용구)\n",
    "\n",
    "    # 모델 및 토크나이저 로드\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "    # 추론 실행\n",
    "    print(f\"Running inference with model: {model_path} ({language})\")\n",
    "    model_predictions = inference(model_path, tokenizer, sentences, system_prompts_en, labels, 'en') # en으로 input language 통일 (학습때도 그랬음)\n",
    "\n",
    "    # 결과 데이터프레임 생성 및 저장\n",
    "    result_df = pd.DataFrame({\n",
    "        'Sentence': sentences,\n",
    "        'Label': labels,  \n",
    "        'Model_Prediction': model_predictions  \n",
    "    })\n",
    "    result_df.to_csv(output_csv_path, index=False)\n",
    "    print(f\"\\nResults saved to {output_csv_path}\")\n",
    "\n",
    "    return result_df\n",
    "\n",
    "# 3. 실행\n",
    "if __name__ == \"__main__\":\n",
    "    model_path = \"/data/uijih/detection/saveded_instruct-full-detection-2\"\n",
    "    #model_path = \"meta-llama/Meta-Llama-3.1-70B-Instruct\"\n",
    "    input_csv_path = \"/data/uijih/previous/Seed50_for_Parallel_Dataset_ENKR_idiomKB_0.8_example.csv\"\n",
    "    output_csv_path_en = \"./8b-inst-full-en-2-trans.csv\"  \n",
    "    output_csv_path_kr = \"./8b-inst-full-kr-2.csv\"  \n",
    "\n",
    "    # 학습 때 사용했던 영어 시스템 프롬프트만 사용 (일단 하나만)\n",
    "    system_prompts_en = [\n",
    "        \"You are a professional translator proficient in Korean and English. Please translate the given English sentence into Korean accurately.\"\n",
    "        # \"Check if the following sentence contains any idioms. If so, return the idiom only. If none, respond with 'none'.\",\n",
    "        # \"Look for an idiom in the following sentence and return the idiom only if found. Otherwise, return 'none'.\"\n",
    "    ]\n",
    "\n",
    "    result_df_en = run_inference_and_save_results(input_csv_path, output_csv_path_en, model_path, system_prompts_en, language='en')    \n",
    "    #result_df_kr = run_inference_and_save_results(input_csv_path, output_csv_path_kr, model_path, system_prompts_en, language='kr')\n",
    "\n",
    "    # Display the DataFrames for visualization in Jupyter\n",
    "    # import ace_tools as tools\n",
    "    # tools.display_dataframe_to_user(name=\"English Inference Results\", dataframe=result_df_en)\n",
    "    # tools.display_dataframe_to_user(name=\"Korean Inference Results\", dataframe=result_df_kr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3.11.3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
