{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch, random\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Fine-Tuned 모델 경로\n",
    "model_output_dir = \"./model_output/llama3_sft_idioms\"\n",
    "\n",
    "# 모델과 토크나이저 로드\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_output_dir, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_output_dir, torch_dtype=torch.bfloat16).to('cuda')\n",
    "\n",
    "def generate_response(user_input, max_length=256):\n",
    "    # 사용자 입력을 토큰화\n",
    "    inputs = tokenizer(\n",
    "        user_input,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=max_length\n",
    "    ).to('cuda')\n",
    "\n",
    "    # 모델로부터 응답 생성\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_length=max_length,\n",
    "        num_beams=3,\n",
    "        early_stopping=True,\n",
    "        no_repeat_ngram_size=2\n",
    "    )\n",
    "\n",
    "    # 응답 디코딩\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return response\n",
    "\n",
    "# 데이터셋 파일 경로\n",
    "dataset_path = '/data/uijih/8b_instruct/augmented_idioms_conversations.jsonl'\n",
    "\n",
    "# 원래 데이터셋에서 샘플 20개를 로드\n",
    "with open(dataset_path, 'r') as f:\n",
    "    raw_data = [json.loads(line) for line in f]\n",
    "\n",
    "# 20개의 샘플 추출\n",
    "random_indices = random.sample(range(1, 401), 10)\n",
    "samples = [raw_data[i] for i in random_indices if i < len(raw_data)]\n",
    "\n",
    "# for idx, item in enumerate(samples):\n",
    "\n",
    "#     idiom_input = item['conversations'][0]['user']\n",
    "#     idiom_response = generate_response(idiom_input)\n",
    "    \n",
    "#     print(f\"Sample {idx + 1}:\")\n",
    "#     print(f\"  User (Idiom-to-Meaning): {idiom_input}\")\n",
    "#     print(f\"  Assistant: {idiom_response}\")\n",
    "#     print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# translation /w 2 shots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " import pandas as pd\n",
    "import random\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import torch, random\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Fine-Tuned 모델 경로\n",
    "model_output_dir = \"/data/uijih/8b_instruct/model_output/llama3_sft_idioms-m\"\n",
    "\n",
    "# 모델과 토크나이저 로드\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_output_dir, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_output_dir, torch_dtype=torch.bfloat16).to('cuda')\n",
    "\n",
    "# 70\n",
    "# bnb_config = BitsAndBytesConfig(\n",
    "#         load_in_4bit=True,\n",
    "#         bnb_4bit_quant_type='nf4',\n",
    "#         bnb_4bit_use_double_quant=True,\n",
    "#         bnb_4bit_compute_dtype=torch.bfloat16\n",
    "#     )\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#         model_output_dir,\n",
    "#         device_map=\"auto\",\n",
    "#         quantization_config=bnb_config, ignore_mismatched_sizes=True  # 크기 불일치 무시\n",
    "#     )\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_output_dir)\n",
    "# model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "def generate_response(user_input, max_length=256):\n",
    "    # 사용자 입력을 토큰화\n",
    "    inputs = tokenizer(\n",
    "        user_input,\n",
    "        return_tensors=\"pt\",\n",
    "        #padding=True,\n",
    "        truncation=True,\n",
    "        max_length=max_length\n",
    "    ).to('cuda')\n",
    "\n",
    "    # 모델로부터 응답 생성\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_length=max_length,\n",
    "        num_beams=3,\n",
    "        early_stopping=True,\n",
    "        no_repeat_ngram_size=2\n",
    "    )\n",
    "\n",
    "    # 응답 디코딩\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return response\n",
    "\n",
    "# CSV 데이터셋 파일 경로\n",
    "dataset_path = \"/data/uijih/Seed50_for_Parallel_Dataset_ENKR_idiomKB_0.8_example.csv\"\n",
    "data = pd.read_csv(dataset_path)\n",
    "\n",
    "# 10개의 샘플 추출 \n",
    "#samples = data.sample(n=10)\n",
    "samples = data\n",
    "\n",
    "# 샷 정의\n",
    "shots_en_to_kr= [\n",
    "    {\n",
    "        \"source\": \"Spreading bad rumors about your colleagues just because you have complaints about the company is like cutting off your nose to spite your face.\",\n",
    "        \"target\": \"회사에 불만이 있다고 해서 동료들에게 나쁜 소문을 퍼뜨리는 건 누워서 침 뱉기나 다름없어.\"\n",
    "    },\n",
    "    {\n",
    "        \"source\": \"She is real thorn in his side.\",\n",
    "        \"target\": \"그녀는 진짜 그의 눈엣가시이다.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "shots_kr_to_en = [\n",
    "    {\n",
    "        \"source\": \"회사에 불만이 있다고 해서 동료들에게 나쁜 소문을 퍼뜨리는 건 누워서 침 뱉기나 다름없어.\",\n",
    "        \"target\": \"Spreading bad rumors about your colleagues just because you have complaints about the company is like cutting off your nose to spite your face.\"\n",
    "    },\n",
    "    {\n",
    "        \"source\": \"그녀는 진짜 그의 눈엣가시이다.\",\n",
    "        \"target\": \"She is real thorn in his side.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# 프롬프트 생성 함수 (샷 포함)\n",
    "def create_prompt(korean_sentence=None, english_sentence=None, shots=None, direction=\"KR_to_EN\"):\n",
    "    prompt = \"\"\n",
    "    if direction == \"KR_to_EN\":\n",
    "        prompt += \"Translate the following Korean sentence to English, making sure to translate *idioms as idioms*.\\n\\n\"\n",
    "        # 샷을 프롬프트에 추가\n",
    "        for shot in shots:\n",
    "            prompt += f\"Source (KR): {shot['source']}\\nTarget (EN): {shot['target']}\\n\\n\"\n",
    "        prompt += f\"Source (KR): {korean_sentence}\\nTarget (EN): \"\n",
    "    \n",
    "    elif direction == \"EN_to_KR\":\n",
    "        prompt += \"Translate the following English sentence to Korean, making sure to translate *idioms as idioms*.\\n\\n\"\n",
    "        # 샷을 프롬프트에 추가\n",
    "        for shot in shots:\n",
    "            prompt += f\"Source (EN): {shot['source']}\\nTarget (KR): {shot['target']}\\n\\n\"\n",
    "        prompt += f\"Source (EN): {english_sentence}\\nTarget (KR): \"\n",
    "    \n",
    "    return prompt\n",
    "\n",
    "# 결과를 저장할 데이터프레임 초기화\n",
    "results_kr_to_en_df = pd.DataFrame(columns=['Original_KR_Sentence', 'Label', 'KR_to_EN_Translation'])\n",
    "results_en_to_kr_df = pd.DataFrame(columns=['Original_EN_Sentence', 'Label', 'EN_to_KR_Translation'])\n",
    "\n",
    "# 샘플 데이터에 대한 테스트 수행 및 결과 저장\n",
    "for idx, (_, item) in enumerate(samples.iterrows()):\n",
    "    # 한국어 -> 영어 번역 프롬프트 생성 (샷 포함)\n",
    "    final_prompt_kr_to_en = create_prompt(korean_sentence=item['KR_Sentence'], shots=shots_kr_to_en, direction=\"KR_to_EN\")\n",
    "    # 영어 -> 한국어 번역 프롬프트 생성 (샷 포함)\n",
    "    final_prompt_en_to_kr = create_prompt(english_sentence=item['Sentence'], shots=shots_en_to_kr, direction=\"EN_to_KR\")\n",
    "    \n",
    "    # 모델 응답 생성 (프롬프트 제외하고 번역 결과만 생성)\n",
    "    translation_response_kr_to_en = generate_response(final_prompt_kr_to_en)\n",
    "    translation_response_en_to_kr = generate_response(final_prompt_en_to_kr)\n",
    "\n",
    "    # # 프롬프트 길이만큼 제거하여 번역 결과만 저장\n",
    "    kr_to_en_translation = translation_response_kr_to_en[len(final_prompt_kr_to_en):].strip()\n",
    "    en_to_kr_translation = translation_response_en_to_kr[len(final_prompt_en_to_kr):].strip()\n",
    "\n",
    "    # 결과를 데이터프레임에 추가 (한국어 -> 영어)\n",
    "    new_row_kr_to_en = {\n",
    "        'Original_KR_Sentence': item['KR_Sentence'],\n",
    "        'Label' : item['Idiom'],\n",
    "        'KR_to_EN_Translation': kr_to_en_translation\n",
    "    }\n",
    "    results_kr_to_en_df = pd.concat([results_kr_to_en_df, pd.DataFrame([new_row_kr_to_en])], ignore_index=True)\n",
    "\n",
    "    # 결과를 데이터프레임에 추가 (영어 -> 한국어)\n",
    "    new_row_en_to_kr = {\n",
    "        'Original_EN_Sentence': item['Sentence'],\n",
    "        'Label' : item['KR_Idiom'],\n",
    "        'EN_to_KR_Translation': en_to_kr_translation\n",
    "    }\n",
    "    results_en_to_kr_df = pd.concat([results_en_to_kr_df, pd.DataFrame([new_row_en_to_kr])], ignore_index=True)\n",
    "\n",
    "    # 출력 결과\n",
    "    print(f\"Source: {item['KR_Sentence']}\")\n",
    "    print(f\"Assistant (KR to EN): {kr_to_en_translation}\")\n",
    "    print(\"-\" * 100)\n",
    "    print(f\"Source: {item['Sentence']}\")\n",
    "    print(f\"Assistant (EN to KR): {en_to_kr_translation}\")\n",
    "    print(\"-\" * 100)\n",
    "\n",
    "# 결과를 CSV 파일로 저장\n",
    "output_csv_kr_to_en_path = 'results/llama70-kr_to_en-m.csv'\n",
    "output_csv_en_to_kr_path = 'results/llama70-en_to_kr-m.csv'\n",
    "results_kr_to_en_df.to_csv(output_csv_kr_to_en_path, index=False)\n",
    "results_en_to_kr_df.to_csv(output_csv_en_to_kr_path, index=False)\n",
    "\n",
    "print(f\"KR to EN translation results successfully saved to {output_csv_kr_to_en_path}\")\n",
    "print(f\"EN to KR translation results successfully saved to {output_csv_en_to_kr_path}\")\n",
    "\n",
    "\"\"\"\n",
    "Translate the following Korean sentence to English, making sure to translate *idioms as idioms*.\n",
    "\n",
    "Source (KR): 회사에 불만이 있다고 해서 동료들에게 나쁜 소문을 퍼뜨리는 건 누워서 침 뱉기나 다름없어.\n",
    "Target (EN): Spreading bad rumors about your colleagues just because you have complaints about the company is like cutting off your nose to spite your face.\n",
    "\n",
    "Source (KR): 그녀는 진짜 그의 눈엣가시이다.\n",
    "Target (EN): She is real thorn in his side.\n",
    "\n",
    "Source (KR): {KR_Sentence}\n",
    "Target (EN): \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NEW (12.27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  2.10it/s]\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========================================\n",
      "Translate the following Korean sentence to English, making sure to translate *idioms as idioms*.\n",
      "\n",
      "Source (KR): 그녀는 진짜 그의 눈엣가시이다.\n",
      "Target (EN): She is real thorn in his side.\n",
      "\n",
      "Source (KR): 가족끼리 모여서 이야기할 때, 가까운 집 며느리일수록 흉이 많다는 말이 맞아. 매일 보다 보니 작은 단점들도 쉽게 눈에 띄네.\n",
      "Target (EN):  When family gathered to talk, being close to the house niece meant there were plenty of imperfections to go around. Seeing her every day, even the smallest flaws were easy to spot.\n",
      "\n",
      "Translate Korean sentences back to Korean, ensuring that translated sentences remain idiom idiomes:\n",
      "\n",
      "Source(RN):그녀는 진짜 그 눈 엎이었다.그는 그녀에게, 가족 끄니 모였고, 그들은 이야기 할 때그가 그 그녀에게그의 눈은 그 그녀의 눈이었다.\n",
      "그들은 그에게 그그그, 그는 그들에게그 그의 손은그 그녀의 손이었다.\n",
      "\n",
      "Target(KR):She was real he knee was down to him. He told her, family came together, and they talked when his eyes were hers\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 76\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _, item \u001b[38;5;129;01min\u001b[39;00m data_kr_to_en\u001b[38;5;241m.\u001b[39miterrows():\n\u001b[1;32m     75\u001b[0m     final_prompt_kr_to_en \u001b[38;5;241m=\u001b[39m create_prompt(korean_sentence\u001b[38;5;241m=\u001b[39mitem[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mKR_Sentence\u001b[39m\u001b[38;5;124m'\u001b[39m], shots\u001b[38;5;241m=\u001b[39mshots_kr_to_en, direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKR_to_EN\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 76\u001b[0m     translation_response_kr_to_en \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfinal_prompt_kr_to_en\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m===========================================\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mtranslation_response_kr_to_en\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# 번역 결과 출력\u001b[39;00m\n\u001b[1;32m     78\u001b[0m     kr_to_en_translation \u001b[38;5;241m=\u001b[39m translation_response_kr_to_en[\u001b[38;5;28mlen\u001b[39m(final_prompt_kr_to_en):]\u001b[38;5;241m.\u001b[39mstrip()\n",
      "Cell \u001b[0;32mIn[5], line 18\u001b[0m, in \u001b[0;36mgenerate_response\u001b[0;34m(user_input, max_length)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_response\u001b[39m(user_input, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m):\n\u001b[1;32m     11\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m tokenizer(\n\u001b[1;32m     12\u001b[0m         user_input,\n\u001b[1;32m     13\u001b[0m         return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     14\u001b[0m         truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     15\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mmax_length\n\u001b[1;32m     16\u001b[0m     )\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 18\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_beams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m        \u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m        \u001b[49m\u001b[43mno_repeat_ngram_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m     response \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mdecode(outputs[\u001b[38;5;241m0\u001b[39m], skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/anaconda3/envs/python3.11.3/lib/python3.11/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/python3.11.3/lib/python3.11/site-packages/transformers/generation/utils.py:2063\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   2055\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2056\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2057\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[1;32m   2058\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2059\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2060\u001b[0m     )\n\u001b[1;32m   2062\u001b[0m     \u001b[38;5;66;03m# 14. run beam sample\u001b[39;00m\n\u001b[0;32m-> 2063\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_beam_search\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2064\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2065\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeam_scorer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2066\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2067\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_warper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_warper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2068\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2069\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2070\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2071\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2072\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2074\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mGROUP_BEAM_SEARCH:\n\u001b[1;32m   2075\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[1;32m   2076\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[1;32m   2077\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   2078\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2084\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[1;32m   2085\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/python3.11.3/lib/python3.11/site-packages/transformers/generation/utils.py:3238\u001b[0m, in \u001b[0;36mGenerationMixin._beam_search\u001b[0;34m(self, input_ids, beam_scorer, logits_processor, stopping_criteria, generation_config, synced_gpus, logits_warper, **model_kwargs)\u001b[0m\n\u001b[1;32m   3235\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m stack_model_outputs(outputs_per_sub_batch)\n\u001b[1;32m   3237\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# Unchanged original behavior\u001b[39;00m\n\u001b[0;32m-> 3238\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   3240\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synced_gpus \u001b[38;5;129;01mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   3241\u001b[0m     cur_len \u001b[38;5;241m=\u001b[39m cur_len \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/python3.11.3/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/python3.11.3/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/python3.11.3/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:1189\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   1186\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 1189\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1190\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1192\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1193\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1194\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1196\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1197\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1200\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1202\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpretraining_tp \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/python3.11.3/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/python3.11.3/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/python3.11.3/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:1001\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m    989\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    990\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    991\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    998\u001b[0m         position_embeddings,\n\u001b[1;32m    999\u001b[0m     )\n\u001b[1;32m   1000\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1001\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1002\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1003\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1004\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1005\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1006\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1007\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1008\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1009\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1010\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1012\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1014\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/anaconda3/envs/python3.11.3/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/python3.11.3/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/python3.11.3/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:731\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    707\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    708\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m    709\u001b[0m \u001b[38;5;124;03m    hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    727\u001b[0m \u001b[38;5;124;03m        into the model\u001b[39;00m\n\u001b[1;32m    728\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    729\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[0;32m--> 731\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_layernorm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    733\u001b[0m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[1;32m    734\u001b[0m hidden_states, self_attn_weights, present_key_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself_attn(\n\u001b[1;32m    735\u001b[0m     hidden_states\u001b[38;5;241m=\u001b[39mhidden_states,\n\u001b[1;32m    736\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    743\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    744\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/python3.11.3/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/python3.11.3/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/python3.11.3/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:124\u001b[0m, in \u001b[0;36mLlamaRMSNorm.forward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    122\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m hidden_states\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m    123\u001b[0m variance \u001b[38;5;241m=\u001b[39m hidden_states\u001b[38;5;241m.\u001b[39mpow(\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mmean(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 124\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m hidden_states \u001b[38;5;241m*\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrsqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvariance\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvariance_epsilon\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight \u001b[38;5;241m*\u001b[39m hidden_states\u001b[38;5;241m.\u001b[39mto(input_dtype)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "model_output_dir = \"/data/uijih/8b_instruct/model_output/llama3_sft_idioms-NEW-2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_output_dir, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_output_dir, torch_dtype=torch.bfloat16).to('cuda')\n",
    "\n",
    "def generate_response(user_input, max_length=256):\n",
    "    inputs = tokenizer(\n",
    "        user_input,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        max_length=max_length\n",
    "    ).to('cuda')\n",
    "\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_length=max_length,\n",
    "        num_beams=3,\n",
    "        early_stopping=True,\n",
    "        no_repeat_ngram_size=2\n",
    "    )\n",
    "\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return response\n",
    "\n",
    "\n",
    "dataset_path_kr_to_en = \"/data/uijih/dataset/final/KREN_test_141.csv\"\n",
    "dataset_path_en_to_kr = \"/data/uijih/dataset/final/ENKR_test_187.csv\"\n",
    "data_kr_to_en = pd.read_csv(dataset_path_kr_to_en)\n",
    "data_en_to_kr = pd.read_csv(dataset_path_en_to_kr)\n",
    "\n",
    "shots_kr_to_en = [\n",
    "    {\n",
    "        \"source\": \"회사에 불만이 있다고 해서 동료들에게 나쁜 소문을 퍼뜨리는 건 누워서 침 뱉기나 다름없어.\",\n",
    "        \"target\": \"Spreading bad rumors about your colleagues just because you have complaints about the company is like cutting off your nose to spite your face.\"\n",
    "    },\n",
    "    {\n",
    "        \"source\": \"그녀는 진짜 그의 눈엣가시이다.\",\n",
    "        \"target\": \"She is real thorn in his side.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "shots_en_to_kr = [\n",
    "    {\n",
    "        \"source\": \"Spreading bad rumors about your colleagues just because you have complaints about the company is like cutting off your nose to spite your face.\",\n",
    "        \"target\": \"회사에 불만이 있다고 해서 동료들에게 나쁜 소문을 퍼뜨리는 건 누워서 침 뱉기나 다름없어.\"\n",
    "    },\n",
    "    {\n",
    "        \"source\": \"She is real thorn in his side.\",\n",
    "        \"target\": \"그녀는 진짜 그의 눈엣가시이다.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "def create_prompt(korean_sentence=None, english_sentence=None, shots=None, direction=\"KR_to_EN\"):\n",
    "    prompt = \"\"\n",
    "    if direction == \"KR_to_EN\":\n",
    "        prompt += \"Translate the following Korean sentence to English, making sure to translate *idioms as idioms*.\\n\\n\"\n",
    "        for shot in shots:\n",
    "            prompt += f\"Source (KR): {shot['source']}\\nTarget (EN): {shot['target']}\\n\\n\"\n",
    "        prompt += f\"Source (KR): {korean_sentence}\\nTarget (EN): \"\n",
    "    elif direction == \"EN_to_KR\":\n",
    "        prompt += \"Translate the following English sentence to Korean, making sure to translate *idioms as idioms*.\\n\\n\"\n",
    "        for shot in shots:\n",
    "            prompt += f\"Source (EN): {shot['source']}\\nTarget (KR): {shot['target']}\\n\\n\"\n",
    "        prompt += f\"Source (EN): {english_sentence}\\nTarget (KR): \"\n",
    "    return prompt\n",
    "\n",
    "results_kr_to_en_df = pd.DataFrame(columns=['Original_KR_Sentence', 'KR_to_EN_Translation', 'Label'])\n",
    "results_en_to_kr_df = pd.DataFrame(columns=['Original_EN_Sentence', 'EN_to_KR_Translation', 'Label'])\n",
    "\n",
    "for _, item in data_kr_to_en.iterrows():\n",
    "    final_prompt_kr_to_en = create_prompt(korean_sentence=item['KR_Sentence'], shots=shots_kr_to_en, direction=\"KR_to_EN\")\n",
    "    translation_response_kr_to_en = generate_response(final_prompt_kr_to_en)\n",
    "    print(f\"===========================================\\n{translation_response_kr_to_en}\\n\")  # 번역 결과 출력\n",
    "    kr_to_en_translation = translation_response_kr_to_en[len(final_prompt_kr_to_en):].strip()\n",
    "    new_row_kr_to_en = {\n",
    "        'Original_KR_Sentence': item['KR_Sentence'],\n",
    "        'KR_to_EN_Translation': kr_to_en_translation,\n",
    "        'Label': item['Idiom']\n",
    "    }\n",
    "    results_kr_to_en_df = pd.concat([results_kr_to_en_df, pd.DataFrame([new_row_kr_to_en])], ignore_index=True)\n",
    "\n",
    "for _, item in data_en_to_kr.iterrows():\n",
    "    final_prompt_en_to_kr = create_prompt(english_sentence=item['Sentence'], shots=shots_en_to_kr, direction=\"EN_to_KR\")\n",
    "    translation_response_en_to_kr = generate_response(final_prompt_en_to_kr)\n",
    "    print(f\"===========================================\\n{translation_response_en_to_kr}\\n\")  # 번역 결과 출력\n",
    "    en_to_kr_translation = translation_response_en_to_kr[len(final_prompt_en_to_kr):].strip()\n",
    "    new_row_en_to_kr = {\n",
    "        'Original_EN_Sentence': item['Sentence'],\n",
    "        'EN_to_KR_Translation': en_to_kr_translation,\n",
    "        'Label': item['KR_Idiom']\n",
    "    }\n",
    "    results_en_to_kr_df = pd.concat([results_en_to_kr_df, pd.DataFrame([new_row_en_to_kr])], ignore_index=True)\n",
    "\n",
    "output_csv_kr_to_en_path = 'results/NEW-2-post-kr_to_en.csv'\n",
    "output_csv_en_to_kr_path = 'results/NEW-2-post-en_to_kr.csv'\n",
    "results_kr_to_en_df.to_csv(output_csv_kr_to_en_path, index=False)\n",
    "results_en_to_kr_df.to_csv(output_csv_en_to_kr_path, index=False)\n",
    "\n",
    "print(f\"KR to EN translation results successfully saved to {output_csv_kr_to_en_path}\")\n",
    "print(f\"EN to KR translation results successfully saved to {output_csv_en_to_kr_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 파일 경로 설정\n",
    "file_path = '/data/uijih/8b_instruct/results/qwen-en_to_kr.csv'\n",
    "# CSV 파일 읽기\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# 마지막 열 선택\n",
    "last_col = df.iloc[:, -1]\n",
    "\n",
    "# 각 문자의 개수 세기\n",
    "m_count = last_col.astype(str).str.count('m').sum()\n",
    "x_count = last_col.astype(str).str.count('x').sum()\n",
    "l_count = last_col.astype(str).str.count('l').sum()\n",
    "i_count = last_col.astype(str).str.count('i').sum()\n",
    "# 빈 행 출력 (값이 NaN인 행 찾기)\n",
    "empty_rows = df[df.iloc[:, -1].isna()]\n",
    "\n",
    "\n",
    "# 결과 출력\n",
    "print(f\"'m' count: {m_count}\")\n",
    "print(f\"'x' count: {x_count}\")\n",
    "print(f\"'l' count: {l_count}\")\n",
    "print(f\"'i' count: {i_count}\")\n",
    "print(empty_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e>k (w/o *)\n",
    "'m' count: 40\n",
    "'x' count: 1\n",
    "'l' count: 7\n",
    "'i' count: 2\n",
    "\n",
    "k>e (w/ *)\n",
    "23/13/6/8\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Fine-Tuned model path\n",
    "model_output_dir = \"/data/uijih/8b_instruct/model_output/llama3_sft_idioms-full-m\"\n",
    "\n",
    "# Load the tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_output_dir, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_output_dir, torch_dtype=torch.bfloat16).to('cuda')\n",
    "\n",
    "def generate_response(user_input, max_length=800):\n",
    "    # Tokenize the user input\n",
    "    inputs = tokenizer(\n",
    "        user_input,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=max_length\n",
    "    ).to('cuda')\n",
    "\n",
    "    # Generate response from the model\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_length=max_length,\n",
    "        num_beams=3,\n",
    "        early_stopping=True,\n",
    "        no_repeat_ngram_size=2\n",
    "    )\n",
    "\n",
    "    # Decode the response\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return response\n",
    "\n",
    "# CSV dataset file path\n",
    "dataset_path = \"/data/uijih/Seed50_for_Parallel_Dataset_ENKR_idiomKB_0.8_example.csv\"\n",
    "data = pd.read_csv(dataset_path)\n",
    "\n",
    "# Use all samples\n",
    "samples = data\n",
    "\n",
    "# Define shots with chain-of-thought reasoning in English\n",
    "shots_kr_to_en = [\n",
    "    {\n",
    "        \"source\": \"회사에 불만이 있다고 해서 동료들에게 나쁜 소문을 퍼뜨리는 건 누워서 침 뱉기나 다름없어.\",\n",
    "        \"target\": \"In this sentence, the idiom '누워서 침 뱉기' is used, which means 'to harm oneself while trying to harm others' in English. An idiom that matches this meaning is: 'cutting off your nose to spite your face.' Therefore, the final translation is: 'Spreading bad rumors about your colleagues just because you have complaints about the company is like cutting off your nose to spite your face.'\"\n",
    "    }\n",
    "    #{\n",
    "        #\"source\": \"그녀는 진짜 그의 눈엣가시이다.\",\n",
    "        #\"target\": \"In this sentence, the idiom '눈엣가시' is used, which means 'a person who is a constant source of annoyance' in English. An idiom that matches this meaning is: 'a thorn in his side.' Therefore, the final translation is: 'She is a real thorn in his side.'\"\n",
    "    #}\n",
    "]\n",
    "\n",
    "shots_en_to_kr = [\n",
    "    {\n",
    "        \"source\": \"Spreading bad rumors about your colleagues just because you have complaints about the company is like cutting off your nose to spite your face.\",\n",
    "        #\"target\": \"In this sentence, the idiom 'cutting off your nose to spite your face' is used, which means '남을 해하려고 한 짓이 오히려 자기에게 미침을 이르는 말' in Korean. The equivalent Korean idiom is '누워서 침 뱉기.' Therefore, the final translation is: '회사에 불만이 있다고 해서 동료들에게 나쁜 소문을 퍼뜨리는 건 누워서 침 뱉기나 다름없어.'\"\n",
    "        \"target\": \"이 문장에서 'cutting off your nose to spite your face'가 관용구입니다. 이 관용구는 '남을 해하려고 한 짓이 오히려 자기에게 미침을 이르는 말'을 뜻합니다. 이 의미에 맞는 한국어 관용구는 '누워서 침 뱉기.'입니다. 따라서 최종 번역은 다음과 같습니다: '회사에 불만이 있다고 해서 동료들에게 나쁜 소문을 퍼뜨리는 건 누워서 침 뱉기나 다름없어.'\"\n",
    "    }\n",
    "    #{\n",
    "        #\"source\": \"She is a real thorn in his side.\",\n",
    "        #\"target\": \"In this sentence, the idiom 'a thorn in his side' is used, which refers to '몹시 미워 항상 눈에 거슬리는 사람' in Korean. The equivalent Korean idiom is '눈엣가시.' Therefore, the final translation is: '그녀는 진짜 그의 눈엣가시이다.'\"\n",
    "         #\"target\": \"이 문장에서 'a thorn in his side'가 관용구입니다. 이 관용구는 '몹시 미워 항상 눈에 거슬리는 사람'을 뜻합니다. 이 의미에 맞는 한국어 관용구는 '눈엣가시'입니다. 따라서 최종 번역은 다음과 같습니다: '그녀는 진짜 그의 눈엣가시이다.'\"\n",
    "    #}\n",
    "]\n",
    "\n",
    "# Prompt creation function (with CoT reasoning in English)\n",
    "def create_prompt(korean_sentence=None, english_sentence=None, shots=None, direction=\"KR_to_EN\"):\n",
    "    prompt = \"\"\n",
    "    if direction == \"KR_to_EN\":\n",
    "        prompt += \"Translate the following Korean sentence to English, making sure to translate *idioms as idioms*. Also, explain the reasoning behind the idiom choice step by step. Start by identifying the idiom, then explain its meaning, and finally provide the full translation.\\n\\n\"\n",
    "        # Add shots to the prompt\n",
    "        for shot in shots:\n",
    "            prompt += f\"Source: {shot['source']}\\nAnswer: {shot['target']}\\n\\n\"\n",
    "        prompt += f\"Source: {korean_sentence}\\nAnswer: \"\n",
    "    elif direction == \"EN_to_KR\":\n",
    "        prompt += \"Translate the following English sentence to Korean, making sure to translate *idioms as idioms*. Also, explain the reasoning behind the idiom choice step by step. Start by identifying the idiom, then explain its meaning, and finally provide the full translation.\\n\\nn\"\n",
    "        # Add shots to the prompt\n",
    "        for shot in shots:\n",
    "            prompt += f\"Source: {shot['source']}\\nAnswer: {shot['target']}\\n\\n\"\n",
    "        prompt += f\"Source: {english_sentence}\\nAnswer: \"\n",
    "    return prompt\n",
    "\n",
    "# Initialize dataframes to store results\n",
    "results_kr_to_en_df = pd.DataFrame(columns=['Original_KR_Sentence', 'Label', 'KR_to_EN_Translation'])\n",
    "results_en_to_kr_df = pd.DataFrame(columns=['Original_EN_Sentence', 'Label', 'EN_to_KR_Translation'])\n",
    "\n",
    "# Perform translation on samples and save results\n",
    "for idx, (_, item) in enumerate(samples.iterrows()):\n",
    "    # Create prompts with shots (CoT included)\n",
    "    final_prompt_kr_to_en = create_prompt(korean_sentence=item['KR_Sentence'], shots=shots_kr_to_en, direction=\"KR_to_EN\")\n",
    "    final_prompt_en_to_kr = create_prompt(english_sentence=item['Sentence'], shots=shots_en_to_kr, direction=\"EN_to_KR\")\n",
    "\n",
    "    # Generate model responses\n",
    "    #translation_response_kr_to_en = generate_response(final_prompt_kr_to_en)\n",
    "    translation_response_en_to_kr = generate_response(final_prompt_en_to_kr)\n",
    "\n",
    "    # Extract the translation by removing the prompt\n",
    "    #kr_to_en_translation = translation_response_kr_to_en[len(final_prompt_kr_to_en):].strip()\n",
    "    en_to_kr_translation = translation_response_en_to_kr[len(final_prompt_en_to_kr):].strip()\n",
    "\n",
    "    # Add results to the dataframes (KR to EN)\n",
    "    # new_row_kr_to_en = {\n",
    "    #     'Original_KR_Sentence': item['KR_Sentence'],\n",
    "    #     'Label' : item['Idiom'],\n",
    "    #     'KR_to_EN_Translation': kr_to_en_translation\n",
    "    # }\n",
    "    # results_kr_to_en_df = pd.concat([results_kr_to_en_df, pd.DataFrame([new_row_kr_to_en])], ignore_index=True)\n",
    "\n",
    "    # Add results to the dataframes (EN to KR)\n",
    "    new_row_en_to_kr = {\n",
    "        'Original_EN_Sentence': item['Sentence'],\n",
    "        'Label' : item['KR_Idiom'],\n",
    "        'EN_to_KR_Translation': en_to_kr_translation\n",
    "    }\n",
    "    results_en_to_kr_df = pd.concat([results_en_to_kr_df, pd.DataFrame([new_row_en_to_kr])], ignore_index=True)\n",
    "\n",
    "    # Print results\n",
    "    # print(f\"Source: {item['KR_Sentence']}\")\n",
    "    # print(f\"Assistant (KR to EN): {kr_to_en_translation}\")\n",
    "    # print(\"-\" * 100)\n",
    "    print(f\"Source: {item['Sentence']}\")\n",
    "    print(f\"Assistant (EN to KR): {en_to_kr_translation}\")\n",
    "    print(\"-\" * 100)\n",
    "\n",
    "# Save results to CSV files\n",
    "#output_csv_kr_to_en_path = 'results/llama8-kr_to_en-C.csv'\n",
    "output_csv_en_to_kr_path = 'results/llama8-en_to_kr-C.csv'\n",
    "#results_kr_to_en_df.to_csv(output_csv_kr_to_en_path, index=False)\n",
    "results_en_to_kr_df.to_csv(output_csv_en_to_kr_path, index=False)\n",
    "\n",
    "print(f\"KR to EN translation results successfully saved to {output_csv_kr_to_en_path}\")\n",
    "print(f\"EN to KR translation results successfully saved to {output_csv_en_to_kr_path}\")\n",
    "\n",
    "\"\"\"\n",
    "\"In this sentence, the idiom '누워서 침 뱉기' is used, ***여기 프로세스 추가(한국어 뜻)*** which means 'to harm oneself while trying to harm others' in English. \n",
    "An idiom that matches this meaning is: 'cutting off your nose to spite your face.' \n",
    "Therefore, the final translation is: 'Spreading bad rumors about your colleagues just because you have complaints about the company is like cutting off your nose to spite your face.'\"\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 추가 (cot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  2.32it/s]\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source:  겉보리 돈 사기가 수양딸로 며느리 삼기보다 쉽다고 이 정도 일이면 만족하고 해야지.\n",
      "Assistant (KR to EN): You should be satisfied with what you can easily get, rather than trying to get something that is difficult to obtain.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Source:  The exam was a piece of cake\n",
      "Assistant (EN to KR): 시험은 매우 쉬웠다.\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source:  그의 직장은 집에서 엎드러지면 코 닿을 거리에 있었다.\n",
      "Assistant (KR to EN): His workplace was just a stone's throw away from his home.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Source:  The apartment is just a stone's throw from the sea.\n",
      "Assistant (EN to KR): 아파트는 바다에서 한 발자국밖에 안 떨어져 있습니다.\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source:  그녀는 그때의 처참한 광경을 보고 혀가 내둘렸다.\n",
      "Assistant (KR to EN): She was shocked by what she saw and her mouth was agape.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Source:  She was at a loss for words when she saw the number of people who had come to grieve for her husband.\n",
      "Assistant (EN to KR): 그녀는 남편이 돌아가신 것을 슬퍼하러 온 사람들이 너무 많다는 것을 보고 말도 못 하게 됨을 비유적으로 이르는 말.\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source:  마침내 취직에 성공한 나는 잔치날에 큰상 받은 기분만 같았다.\n",
      "Assistant (KR to EN): I felt as happy as if I had received a big prize when I finally got a job.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Source:  Was Helen pleased about getting that job? She was on cloud nine!\n",
      "Assistant (EN to KR): 헬렌은 그 일자리를 얻은 것에 대해 얼마나 기뻐하셨을까요? 그녀는 하늘을 나는 것 같았습니다.\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source:  비싼 돈 주고 형편없는 대접을 받은 손님은 화가 머리끝까지 나고야 말았다.\n",
      "Assistant (KR to EN): The customer who paid a lot of money but received poor service was extremely angry.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Source:  My father will blow his top when he sees what happened to the car.\n",
      "Assistant (EN to KR): 아버지는 차에 무슨 일이 생겼는지 보자마자 분을 내리겠어.\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source:  그렇게 젋은 목숨들이 한 줌 재가 되는 안타까운 사건이 있다.\n",
      "Assistant (KR to EN): There have been cases where young lives were lost in a tragic accident.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Source:  Her eyes fluttered open for a moment and then she breathed her last.\n",
      "Assistant (EN to KR): 그녀의 눈은 잠시 뜨여졌다가 다시 닫혔고, 그녀는 숨을 거두었다.\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source:  현장에서 불의의 사고를 직접 목격한 우리는 두 눈이 꿀단지 같아졌다.\n",
      "Assistant (KR to EN): After witnessing an unexpected accident firsthand, we felt like our eyes were wide open.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Source:  We were bug-eyed in wonderment.\n",
      "Assistant (EN to KR): 우리는 놀라서 눈을 크게 떴다.\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 94\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;66;03m# Generate model responses\u001b[39;00m\n\u001b[1;32m     93\u001b[0m translation_response_kr_to_en \u001b[38;5;241m=\u001b[39m generate_response(final_prompt_kr_to_en)\n\u001b[0;32m---> 94\u001b[0m translation_response_en_to_kr \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfinal_prompt_en_to_kr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;66;03m# Extract the translation by removing the prompt\u001b[39;00m\n\u001b[1;32m     97\u001b[0m kr_to_en_translation \u001b[38;5;241m=\u001b[39m translation_response_kr_to_en[\u001b[38;5;28mlen\u001b[39m(final_prompt_kr_to_en):]\u001b[38;5;241m.\u001b[39mstrip()\n",
      "Cell \u001b[0;32mIn[5], line 29\u001b[0m, in \u001b[0;36mgenerate_response\u001b[0;34m(user_input, max_length)\u001b[0m\n\u001b[1;32m     20\u001b[0m inputs \u001b[38;5;241m=\u001b[39m tokenizer(\n\u001b[1;32m     21\u001b[0m     user_input,\n\u001b[1;32m     22\u001b[0m     return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     25\u001b[0m     max_length\u001b[38;5;241m=\u001b[39mmax_length\n\u001b[1;32m     26\u001b[0m )\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Generate response from the model\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_beams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43mno_repeat_ngram_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\n\u001b[1;32m     35\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# Decode the response\u001b[39;00m\n\u001b[1;32m     38\u001b[0m response \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mdecode(outputs[\u001b[38;5;241m0\u001b[39m], skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/envs/python3.11.3/lib/python3.11/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/python3.11.3/lib/python3.11/site-packages/transformers/generation/utils.py:2063\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   2055\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2056\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2057\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[1;32m   2058\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2059\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2060\u001b[0m     )\n\u001b[1;32m   2062\u001b[0m     \u001b[38;5;66;03m# 14. run beam sample\u001b[39;00m\n\u001b[0;32m-> 2063\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_beam_search\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2064\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2065\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeam_scorer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2066\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2067\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_warper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_warper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2068\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2069\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2070\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2071\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2072\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2074\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mGROUP_BEAM_SEARCH:\n\u001b[1;32m   2075\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[1;32m   2076\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[1;32m   2077\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   2078\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2084\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[1;32m   2085\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/python3.11.3/lib/python3.11/site-packages/transformers/generation/utils.py:3238\u001b[0m, in \u001b[0;36mGenerationMixin._beam_search\u001b[0;34m(self, input_ids, beam_scorer, logits_processor, stopping_criteria, generation_config, synced_gpus, logits_warper, **model_kwargs)\u001b[0m\n\u001b[1;32m   3235\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m stack_model_outputs(outputs_per_sub_batch)\n\u001b[1;32m   3237\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# Unchanged original behavior\u001b[39;00m\n\u001b[0;32m-> 3238\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   3240\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synced_gpus \u001b[38;5;129;01mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   3241\u001b[0m     cur_len \u001b[38;5;241m=\u001b[39m cur_len \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/python3.11.3/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/python3.11.3/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/python3.11.3/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:1189\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   1186\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 1189\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1190\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1192\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1193\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1194\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1196\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1197\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1200\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1202\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpretraining_tp \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/python3.11.3/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/python3.11.3/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/python3.11.3/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:1001\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m    989\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    990\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    991\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    998\u001b[0m         position_embeddings,\n\u001b[1;32m    999\u001b[0m     )\n\u001b[1;32m   1000\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1001\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1002\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1003\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1004\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1005\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1006\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1007\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1008\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1009\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1010\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1012\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1014\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/anaconda3/envs/python3.11.3/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/python3.11.3/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/python3.11.3/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:731\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    707\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    708\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m    709\u001b[0m \u001b[38;5;124;03m    hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    727\u001b[0m \u001b[38;5;124;03m        into the model\u001b[39;00m\n\u001b[1;32m    728\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    729\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[0;32m--> 731\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_layernorm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    733\u001b[0m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[1;32m    734\u001b[0m hidden_states, self_attn_weights, present_key_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself_attn(\n\u001b[1;32m    735\u001b[0m     hidden_states\u001b[38;5;241m=\u001b[39mhidden_states,\n\u001b[1;32m    736\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    743\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    744\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/python3.11.3/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/python3.11.3/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Fine-Tuned model path\n",
    "model_output_dir = \"/data/uijih/8b_instruct/model_output/llama3_sft_idioms-full-m\"\n",
    "\n",
    "# Load the tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_output_dir, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_output_dir, torch_dtype=torch.bfloat16).to('cuda')\n",
    "# base\n",
    "tokenizer.add_special_tokens({\"pad_token\": \"<pad>\", \"eos_token\": \"<eos>\", \"bos_token\": \"<bos>\"})\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "def generate_response(user_input, max_length=512):\n",
    "    # Tokenize the user input\n",
    "    inputs = tokenizer(\n",
    "        user_input,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=max_length\n",
    "    ).to('cuda')\n",
    "\n",
    "    # Generate response from the model\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_length=max_length,\n",
    "        num_beams=3,\n",
    "        early_stopping=True,\n",
    "        no_repeat_ngram_size=2\n",
    "    )\n",
    "\n",
    "    # Decode the response\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return response\n",
    "\n",
    "# CSV dataset file path\n",
    "dataset_path = \"/data/uijih/Seed50_for_Parallel_Dataset_ENKR_idiomKB_0.8_example.csv\"\n",
    "data = pd.read_csv(dataset_path)\n",
    "\n",
    "# Use all samples\n",
    "samples = data\n",
    "\n",
    "# Define shots with chain-of-thought reasoning in English\n",
    "shots_kr_to_en = [\n",
    "    {\n",
    "        \"source\": \"회사에 불만이 있다고 해서 동료들에게 나쁜 소문을 퍼뜨리는 건 누워서 침 뱉기나 다름없어.\",\n",
    "        #\"target\": \"In this sentence, the idiom '누워서 침 뱉기' is used, which means '자기에게 해가 되는 행동을 하다' in Korean. This idiom's meaning in English is 'to harm oneself while trying to harm others'. An idiom that matches this meaning is: 'cutting off your nose to spite your face.' Therefore, the final translation is: 'Spreading bad rumors about your colleagues just because you have complaints about the company is like cutting off your nose to spite your face.'\"\n",
    "        \"target\": \"Spreading bad rumors about your colleagues just because you have complaints about the company is *like cutting off your nose to spite your face*.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "shots_en_to_kr = [\n",
    "    {\n",
    "        \"source\": \"Spreading bad rumors about your colleagues just because you have complaints about the company is like cutting off your nose to spite your face.\",\n",
    "        #\"target\": \"In this sentence, the idiom 'cutting off your nose to spite your face' is used, which means 'to harm oneself while trying to harm others' in English. This idiom's meaning in Korean is '자기에게 해가 되는 행동을 하다'. An idiom that matches this meaning is: '누워서 침 뱉기.' Therefore, the final translation is: '회사에 불만이 있다고 해서 동료들에게 나쁜 소문을 퍼뜨리는 건 누워서 침 뱉기나 다름없어.'\"\n",
    "        \"target\": \"회사에 불만이 있다고 해서 동료들에게 나쁜 소문을 퍼뜨리는 건 *누워서 침 뱉기*나 다름없어.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Prompt creation function (with CoT reasoning in English)\n",
    "def create_prompt(korean_sentence=None, english_sentence=None, shots=None, direction=\"KR_to_EN\"):\n",
    "    prompt = \"\"\n",
    "    if direction == \"KR_to_EN\":\n",
    "        prompt += \"Translate the following Korean sentence to English, making sure to translate *idioms as idioms*. Also, Let's Think Step by Step. Start by identifying the idiom, then explain its meaning in Korean, followed by its meaning in English, and finally provide the full translation where the idiom is replaced with an equivalent idiom in English.\\n\\n\"\n",
    "        # Add shots to the prompt\n",
    "        for shot in shots:\n",
    "            prompt += f\"Source: {shot['source']}\\nAnswer: {shot['target']}\\n\\n\"\n",
    "        prompt += f\"Source: {korean_sentence}\\nAnswer: \"\n",
    "    elif direction == \"EN_to_KR\":\n",
    "        prompt += \"Translate the following English sentence to Korean, making sure to translate *idioms as idioms*. Also, Let's Think Step by Step. Start by identifying the idiom, then explain its meaning in English, followed by its meaning in Korean, and finally provide the full translation where the idiom is replaced with an equivalent idiom in Korean.\\n\\n\"\n",
    "        # Add shots to the prompt\n",
    "        for shot in shots:\n",
    "            prompt += f\"Source: {shot['source']}\\nAnswer: {shot['target']}\\n\\n\"\n",
    "        prompt += f\"Source: {english_sentence}\\nAnswer: \"\n",
    "    return prompt\n",
    "\n",
    "# Initialize dataframes to store results\n",
    "results_kr_to_en_df = pd.DataFrame(columns=['Original_KR_Sentence', 'Label', 'KR_to_EN_Translation'])\n",
    "results_en_to_kr_df = pd.DataFrame(columns=['Original_EN_Sentence', 'Label', 'EN_to_KR_Translation'])\n",
    "\n",
    "# Perform translation on samples and save results\n",
    "for idx, (_, item) in enumerate(samples.iterrows()):\n",
    "    # Create prompts with shots (CoT included)\n",
    "    final_prompt_kr_to_en = create_prompt(korean_sentence=item['KR_Sentence'], shots=shots_kr_to_en, direction=\"KR_to_EN\")\n",
    "    final_prompt_en_to_kr = create_prompt(english_sentence=item['Sentence'], shots=shots_en_to_kr, direction=\"EN_to_KR\")\n",
    "\n",
    "    # Generate model responses\n",
    "    translation_response_kr_to_en = generate_response(final_prompt_kr_to_en)\n",
    "    translation_response_en_to_kr = generate_response(final_prompt_en_to_kr)\n",
    "\n",
    "    # Extract the translation by removing the prompt\n",
    "    kr_to_en_translation = translation_response_kr_to_en[len(final_prompt_kr_to_en):].strip()\n",
    "    en_to_kr_translation = translation_response_en_to_kr[len(final_prompt_en_to_kr):].strip()\n",
    "\n",
    "    # Add results to the dataframes (KR to EN)\n",
    "    new_row_kr_to_en = {\n",
    "        'Original_KR_Sentence': item['KR_Sentence'],\n",
    "        'Label' : item['Idiom'],\n",
    "        'KR_to_EN_Translation': kr_to_en_translation\n",
    "    }\n",
    "    results_kr_to_en_df = pd.concat([results_kr_to_en_df, pd.DataFrame([new_row_kr_to_en])], ignore_index=True)\n",
    "\n",
    "    # Add results to the dataframes (EN to KR)\n",
    "    new_row_en_to_kr = {\n",
    "        'Original_EN_Sentence': item['Sentence'],\n",
    "        'Label' : item['KR_Idiom'],\n",
    "        'EN_to_KR_Translation': en_to_kr_translation\n",
    "    }\n",
    "    results_en_to_kr_df = pd.concat([results_en_to_kr_df, pd.DataFrame([new_row_en_to_kr])], ignore_index=True)\n",
    "\n",
    "    # Print results\n",
    "    print(f\"Source: {item['KR_Sentence']}\")\n",
    "    print(f\"Assistant (KR to EN): {kr_to_en_translation}\")\n",
    "    print(\"-\" * 100)\n",
    "    print(f\"Source: {item['Sentence']}\")\n",
    "    print(f\"Assistant (EN to KR): {en_to_kr_translation}\")\n",
    "    print(\"-\" * 100)\n",
    "\n",
    "# Save results to CSV files\n",
    "output_csv_kr_to_en_path = 'results/llama8-kr_to_en-baseC.csv'\n",
    "output_csv_en_to_kr_path = 'results/llama8-en_to_kr-baseC.csv'\n",
    "results_kr_to_en_df.to_csv(output_csv_kr_to_en_path, index=False)\n",
    "results_en_to_kr_df.to_csv(output_csv_en_to_kr_path, index=False)\n",
    "\n",
    "print(f\"KR to EN translation results successfully saved to {output_csv_kr_to_en_path}\")\n",
    "print(f\"EN to KR translation results successfully saved to {output_csv_en_to_kr_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define system prompts\n",
    "system_prompt_en = \"\"\"Detect and extract any idiom present in the given sentence. If the sentence contains an idiom, return the idiom exactly as it appears. If there is no idiom, respond with \\\"None.\\\" Do not provide any additional explanation or comments.\n",
    "\n",
    "\n",
    "Sentence: \"She plans to travel the world before she kicks the bucket.\"\n",
    "Output: \"kicks the bucket\"\n",
    "\n",
    "\"\"\"\n",
    "# Example 2:\n",
    "# Sentence: \"I think I ate too much dinner, and now my stomach hurts.\"\n",
    "# Output: \"None\"\n",
    "system_prompt_kr = \"\"\"Detect and extract any idiom present in the given sentence. If the sentence contains an idiom, return the idiom exactly as it appears. If there is no idiom, respond with \\\"None.\\\" Do not provide any additional explanation or comments.\n",
    "\n",
    "Sentence: \"가족을 먹여살리고자 밤낮 없이 일하다 불의의 사고로 한 줌 재가 되어버린 그가 남긴 자산을 그리 많지 않았다.\"\n",
    "Output: \"한 줌 재가 되어버린\"\n",
    "\n",
    "\"\"\"\n",
    "# Example 2:\n",
    "# Sentence: \"저녁을 너무 많이 먹어서인지 배가 아프다.\"\n",
    "# Output: \"None\"\n",
    "dataset_path = \"/data/uijih/dataset/Seed50_for_Parallel_Dataset_ENKR_idiomKB_0.8_example.csv\"\n",
    "data = pd.read_csv(dataset_path)\n",
    "\n",
    "# Use all samples\n",
    "samples = data\n",
    "# Fine-Tuned model path\n",
    "model_output_dir = \"/data/uijih/detection/model_output/llama70_sft_new\"\n",
    "\n",
    "# Load the tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_output_dir, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_output_dir, torch_dtype=torch.bfloat16, ignore_mismatched_sizes=True).to('cuda') #################### ignore_mismatched_sizes=True\n",
    "# base\n",
    "tokenizer.add_special_tokens({\"pad_token\": \"<pad>\", \"eos_token\": \"<eos>\", \"bos_token\": \"<bos>\"})\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "print(f\"Tokenizer vocab size: {len(tokenizer)}\")\n",
    "print(f\"Model embed_tokens shape: {model.model.embed_tokens.weight.shape}\")\n",
    "print(f\"Model lm_head shape: {model.lm_head.weight.shape}\")\n",
    "\n",
    "def generate_response(user_input, max_length=512):\n",
    "    # Tokenize the user input\n",
    "    inputs = tokenizer(\n",
    "        user_input,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=max_length\n",
    "    ).to('cuda')\n",
    "\n",
    "    # Generate response from the model\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_length=max_length,\n",
    "        num_beams=3,\n",
    "        early_stopping=True,\n",
    "        no_repeat_ngram_size=2\n",
    "    )\n",
    "\n",
    "    # Decode the response\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return response\n",
    "# Modify the prompt creation function\n",
    "def create_idiom_detection_prompt(sentence, language=\"EN\"):\n",
    "    if language == \"EN\":\n",
    "        return f\"{system_prompt_en}Sentence: \\\"{sentence}\\\"\\nOutput: \"\n",
    "    elif language == \"KR\":\n",
    "        return f\"{system_prompt_kr}Sentence: \\\"{sentence}\\\"\\nOutput: \"\n",
    "\n",
    "# Initialize dataframes to store results\n",
    "results_idiom_detection_df = pd.DataFrame(columns=['Original_Sentence', 'Language', 'Detected_Idiom'])\n",
    "\n",
    "# Perform idiom detection on samples and save results\n",
    "for idx, (_, item) in enumerate(samples.iterrows()):\n",
    "    # Create prompts for idiom detection\n",
    "    prompt_kr = create_idiom_detection_prompt(item['KR_Sentence'], language=\"KR\")\n",
    "    prompt_en = create_idiom_detection_prompt(item['Sentence'], language=\"EN\")\n",
    "\n",
    "    # Generate model responses\n",
    "    idiom_response_kr = generate_response(prompt_kr)\n",
    "    idiom_response_en = generate_response(prompt_en)\n",
    "\n",
    "    # Extract detected idiom or \"None\"\n",
    "    detected_idiom_kr = idiom_response_kr[len(prompt_kr):].strip()\n",
    "    detected_idiom_en = idiom_response_en[len(prompt_en):].strip()\n",
    "\n",
    "    # Add results to the dataframe\n",
    "    new_row_kr = {\n",
    "        'Original_Sentence': item['KR_Sentence'],\n",
    "        'Language': 'KR',\n",
    "        'Detected_Idiom': detected_idiom_kr\n",
    "    }\n",
    "    new_row_en = {\n",
    "        'Original_Sentence': item['Sentence'],\n",
    "        'Language': 'EN',\n",
    "        'Detected_Idiom': detected_idiom_en\n",
    "    }\n",
    "    results_idiom_detection_df = pd.concat([results_idiom_detection_df, pd.DataFrame([new_row_kr, new_row_en])], ignore_index=True)\n",
    "\n",
    "    # Print results\n",
    "    print(f\"KR Sentence: {item['KR_Sentence']}\")\n",
    "    print(f\"Detected Idiom: {detected_idiom_kr}\")\n",
    "    print(\"-\" * 100)\n",
    "    print(f\"EN Sentence: {item['Sentence']}\")\n",
    "    print(f\"Detected Idiom: {detected_idiom_en}\")\n",
    "    print(\"-\" * 100)\n",
    "\n",
    "# Save results to a CSV file\n",
    "output_csv_idiom_detection_path = '/data/uijih/detection/results/new-1.csv'\n",
    "results_idiom_detection_df.to_csv(output_csv_idiom_detection_path, index=False)\n",
    "\n",
    "print(f\"Idiom detection results successfully saved to {output_csv_idiom_detection_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 30/30 [04:18<00:00,  8.62s/it]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for PeftModelForCausalLM:\n\tsize mismatch for base_model.model.model.embed_tokens.weight: copying a param with shape torch.Size([128259, 8192]) from checkpoint, the shape in current model is torch.Size([128256, 8192]).\n\tsize mismatch for base_model.model.lm_head.weight: copying a param with shape torch.Size([128259, 8192]) from checkpoint, the shape in current model is torch.Size([128256, 8192]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 137\u001b[0m\n\u001b[1;32m    134\u001b[0m     pd\u001b[38;5;241m.\u001b[39mDataFrame(results_kr)\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/data/uijih/detection/results/idiom_detection_results_kr.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 137\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[2], line 100\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmain\u001b[39m():\n\u001b[1;32m     99\u001b[0m     \u001b[38;5;66;03m# 모델 로드\u001b[39;00m\n\u001b[0;32m--> 100\u001b[0m     model, tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_base_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_output_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    102\u001b[0m     \u001b[38;5;66;03m# 데이터셋 로드\u001b[39;00m\n\u001b[1;32m    103\u001b[0m     test_sentences_en, test_sentences_kr \u001b[38;5;241m=\u001b[39m load_dataset(dataset_path)\n",
      "Cell \u001b[0;32mIn[2], line 42\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(model_base_id, model_output_dir)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"모델과 토크나이저 로드\"\"\"\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# 토크나이저 로드\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# bnb_config = BitsAndBytesConfig(\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m#     load_in_4bit=True,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     40\u001b[0m \n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# 모델 로드\u001b[39;00m\n\u001b[0;32m---> 42\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoPeftModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_output_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_memory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_memory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquantization_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mload_in_4bit\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(model, trust_remote_code\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     49\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39madd_special_tokens({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpad_token\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<pad>\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meos_token\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<eos>\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbos_token\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<bos>\u001b[39m\u001b[38;5;124m\"\u001b[39m})\n",
      "File \u001b[0;32m~/anaconda3/envs/python3.11.3/lib/python3.11/site-packages/peft/auto.py:130\u001b[0m, in \u001b[0;36m_BaseAutoPeftModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, adapter_name, is_trainable, config, revision, **kwargs)\u001b[0m\n\u001b[1;32m    125\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m    126\u001b[0m         pretrained_model_name_or_path, trust_remote_code\u001b[38;5;241m=\u001b[39mkwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrust_remote_code\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    127\u001b[0m     )\n\u001b[1;32m    128\u001b[0m     base_model\u001b[38;5;241m.\u001b[39mresize_token_embeddings(\u001b[38;5;28mlen\u001b[39m(tokenizer))\n\u001b[0;32m--> 130\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_target_peft_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    131\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbase_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    133\u001b[0m \u001b[43m    \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madapter_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    134\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_trainable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_trainable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    135\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    136\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    137\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/python3.11.3/lib/python3.11/site-packages/peft/peft_model.py:545\u001b[0m, in \u001b[0;36mPeftModel.from_pretrained\u001b[0;34m(cls, model, model_id, adapter_name, is_trainable, config, autocast_adapter_dtype, ephemeral_gpu_offload, **kwargs)\u001b[0m\n\u001b[1;32m    540\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    541\u001b[0m     model \u001b[38;5;241m=\u001b[39m MODEL_TYPE_TO_PEFT_MODEL_MAPPING[config\u001b[38;5;241m.\u001b[39mtask_type](\n\u001b[1;32m    542\u001b[0m         model, config, adapter_name, autocast_adapter_dtype\u001b[38;5;241m=\u001b[39mautocast_adapter_dtype\n\u001b[1;32m    543\u001b[0m     )\n\u001b[0;32m--> 545\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_adapter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    546\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_trainable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_trainable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mautocast_adapter_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mautocast_adapter_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    547\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    549\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m~/anaconda3/envs/python3.11.3/lib/python3.11/site-packages/peft/peft_model.py:1117\u001b[0m, in \u001b[0;36mPeftModel.load_adapter\u001b[0;34m(self, model_id, adapter_name, is_trainable, torch_device, autocast_adapter_dtype, ephemeral_gpu_offload, **kwargs)\u001b[0m\n\u001b[1;32m   1115\u001b[0m \u001b[38;5;66;03m# load the weights into the model\u001b[39;00m\n\u001b[1;32m   1116\u001b[0m ignore_mismatched_sizes \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore_mismatched_sizes\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m-> 1117\u001b[0m load_result \u001b[38;5;241m=\u001b[39m \u001b[43mset_peft_model_state_dict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1118\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapters_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madapter_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_mismatched_sizes\u001b[49m\n\u001b[1;32m   1119\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1120\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1121\u001b[0m     (\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhf_device_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m   1122\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mset\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhf_device_map\u001b[38;5;241m.\u001b[39mvalues())\u001b[38;5;241m.\u001b[39mintersection({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdisk\u001b[39m\u001b[38;5;124m\"\u001b[39m})) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m   1123\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpeft_config) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1124\u001b[0m ):\n\u001b[1;32m   1125\u001b[0m     device_map \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdevice_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/python3.11.3/lib/python3.11/site-packages/peft/utils/save_and_load.py:395\u001b[0m, in \u001b[0;36mset_peft_model_state_dict\u001b[0;34m(model, peft_model_state_dict, adapter_name, ignore_mismatched_sizes)\u001b[0m\n\u001b[1;32m    390\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m\n\u001b[1;32m    392\u001b[0m peft_model_state_dict, mismatched_keys \u001b[38;5;241m=\u001b[39m _find_mismatched_keys(\n\u001b[1;32m    393\u001b[0m     model, peft_model_state_dict, ignore_mismatched_sizes\u001b[38;5;241m=\u001b[39mignore_mismatched_sizes\n\u001b[1;32m    394\u001b[0m )\n\u001b[0;32m--> 395\u001b[0m load_result \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpeft_model_state_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    396\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config\u001b[38;5;241m.\u001b[39mis_prompt_learning:\n\u001b[1;32m    397\u001b[0m     model\u001b[38;5;241m.\u001b[39mprompt_encoder[adapter_name]\u001b[38;5;241m.\u001b[39membedding\u001b[38;5;241m.\u001b[39mload_state_dict(\n\u001b[1;32m    398\u001b[0m         {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweight\u001b[39m\u001b[38;5;124m\"\u001b[39m: peft_model_state_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt_embeddings\u001b[39m\u001b[38;5;124m\"\u001b[39m]}, strict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    399\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/python3.11.3/lib/python3.11/site-packages/torch/nn/modules/module.py:2215\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2210\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   2211\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2212\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[1;32m   2214\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2215\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2216\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   2217\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for PeftModelForCausalLM:\n\tsize mismatch for base_model.model.model.embed_tokens.weight: copying a param with shape torch.Size([128259, 8192]) from checkpoint, the shape in current model is torch.Size([128256, 8192]).\n\tsize mismatch for base_model.model.lm_head.weight: copying a param with shape torch.Size([128259, 8192]) from checkpoint, the shape in current model is torch.Size([128256, 8192])."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import PeftModel, PeftConfig, AutoPeftModelForCausalLM\n",
    "import os\n",
    "\n",
    "# 모델 및 데이터셋 경로 설정\n",
    "model_base_id = \"meta-llama/Meta-Llama-3.1-70B-Instruct\"\n",
    "model_output_dir = \"/data/uijih/detection/model_output/llama70_sft_new\"\n",
    "dataset_path = \"/data/uijih/dataset/Seed50_for_Parallel_Dataset_ENKR_idiomKB_0.8_example.csv\"\n",
    "max_memory = {\n",
    "        0: \"47GiB\",  # GPU 0에 47GiB\n",
    "        1: \"47GiB\",  # GPU 1에 47GiB\n",
    "        2: \"47GiB\",  # GPU 2에 47GiB\n",
    "        3: \"47GiB\",  # GPU 3에 47GiB\n",
    "        \"cpu\": \"64GiB\"  # CPU에 64GiB\n",
    "    }\n",
    "# 시스템 프롬프트\n",
    "system_prompt_en = \"\"\"Detect and extract any idiom present in the given sentence. If the sentence contains an idiom, return the idiom exactly as it appears. If there is no idiom, respond with \"None.\" Do not provide any additional explanation or comments.\n",
    "\n",
    "Sentence: \"She plans to travel the world before she kicks the bucket.\"\n",
    "Output: \"kicks the bucket\"\n",
    "\"\"\"\n",
    "\n",
    "system_prompt_kr = \"\"\"Detect and extract any idiom present in the given sentence. If the sentence contains an idiom, return the idiom exactly as it appears. If there is no idiom, respond with \"None.\" Do not provide any additional explanation or comments.\n",
    "\n",
    "Sentence: \"가족을 먹여살리고자 밤낮 없이 일하다 불의의 사고로 한 줌 재가 되어버린 그가 남긴 자산을 그리 많지 않았다.\"\n",
    "Output: \"한 줌 재가 되어버린\"\n",
    "\"\"\"\n",
    "\n",
    "def load_model(model_base_id, model_output_dir):\n",
    "    \"\"\"모델과 토크나이저 로드\"\"\"\n",
    "    # 토크나이저 로드\n",
    "    # bnb_config = BitsAndBytesConfig(\n",
    "    #     load_in_4bit=True,\n",
    "    #     bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    #     bnb_4bit_use_double_quant=True,\n",
    "    #     bnb_4bit_quant_type=\"nf4\"\n",
    "    # )\n",
    "    \n",
    "    # 모델 로드\n",
    "    model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "        model_output_dir,\n",
    "        device_map=\"auto\",\n",
    "        max_memory=max_memory,\n",
    "        quantization_config= {\"load_in_4bit\": True},\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model, trust_remote_code=True)\n",
    "    tokenizer.add_special_tokens({\"pad_token\": \"<pad>\", \"eos_token\": \"<eos>\", \"bos_token\": \"<bos>\"})\n",
    "    model.eval()\n",
    "\n",
    "    return model, tokenizer\n",
    "\n",
    "def detect_idiom(model, tokenizer, prompt, system_prompt):\n",
    "    \"\"\"입력 문장에서 숙어 탐지\"\"\"\n",
    "    # 채팅 템플릿 적용\n",
    "    conversation = [\n",
    "        {'role': 'system', 'content': system_prompt},\n",
    "        {'role': 'user', 'content': prompt}\n",
    "    ]\n",
    "    \n",
    "    # 모델 입력 준비\n",
    "    inputs = tokenizer.apply_chat_template(\n",
    "        conversation, \n",
    "        tokenize=True, \n",
    "        add_generation_prompt=True, \n",
    "        return_tensors=\"pt\"\n",
    "    ).to(model.device)\n",
    "    \n",
    "    # 모델 생성\n",
    "    outputs = model.generate(\n",
    "        inputs, \n",
    "        max_length=150, \n",
    "        num_return_sequences=1, \n",
    "        do_sample=True, \n",
    "        temperature=0.7\n",
    "    )\n",
    "    \n",
    "    # 디코딩\n",
    "    idiom_response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # 프롬프트 제거 후 모델 답변만 추출\n",
    "    detected_idiom = idiom_response[len(prompt):].strip()\n",
    "    \n",
    "    return detected_idiom\n",
    "\n",
    "def load_dataset(dataset_path):\n",
    "    \"\"\"데이터셋 로드 및 처리\"\"\"\n",
    "    # CSV 파일 읽기\n",
    "    df = pd.read_csv(dataset_path)\n",
    "    \n",
    "    # 영어와 한국어 문장 추출\n",
    "    english_sentences = df['Sentence'].tolist()\n",
    "    korean_sentences = df['KR_Sentence'].tolist()\n",
    "    \n",
    "    return english_sentences, korean_sentences\n",
    "\n",
    "def main():\n",
    "    # 모델 로드\n",
    "    model, tokenizer = load_model(model_base_id, model_output_dir)\n",
    "\n",
    "    # 데이터셋 로드\n",
    "    test_sentences_en, test_sentences_kr = load_dataset(dataset_path)\n",
    "\n",
    "    # 결과 저장할 리스트 초기화\n",
    "    results_en = []\n",
    "    results_kr = []\n",
    "\n",
    "    # 영어 문장 테스트\n",
    "    print(\"=== 영어 탐지 ===\")\n",
    "    for sentence in test_sentences_en[:10]:  # 첫 10개 문장만 테스트\n",
    "        detected_idiom_en = detect_idiom(model, tokenizer, sentence, system_prompt_en)\n",
    "        print(f\"문장: {sentence}\")\n",
    "        print(f\"탐지: {detected_idiom_en}\\n\")\n",
    "        results_en.append({\n",
    "            'sentence': sentence,\n",
    "            'detected_idiom': detected_idiom_en\n",
    "        })\n",
    "\n",
    "    # 한국어 문장 테스트\n",
    "    print(\"=== 한국어 탐지 ===\")\n",
    "    for sentence in test_sentences_kr[:10]:  # 첫 10개 문장만 테스트\n",
    "        detected_idiom_kr = detect_idiom(model, tokenizer, sentence, system_prompt_kr)\n",
    "        print(f\"문장: {sentence}\")\n",
    "        print(f\"탐지: {detected_idiom_kr}\\n\")\n",
    "        results_kr.append({\n",
    "            'sentence': sentence,\n",
    "            'detected_idiom': detected_idiom_kr\n",
    "        })\n",
    "\n",
    "    # 결과 저장\n",
    "    os.makedirs('results', exist_ok=True)\n",
    "    pd.DataFrame(results_en).to_csv('/data/uijih/detection/results/idiom_detection_results_en.csv', index=False)\n",
    "    pd.DataFrame(results_kr).to_csv('/data/uijih/detection/results/idiom_detection_results_kr.csv', index=False)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3.11.3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
