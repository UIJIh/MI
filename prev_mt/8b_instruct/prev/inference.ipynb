{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# 모델과 토크나이저 로드\n",
    "model_path = \"./saveded_instruct-full-1\" \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, local_files_only=True, device_map=\"auto\")\n",
    "model.eval()\n",
    "\n",
    "# Prompt template\n",
    "prompt_template = \"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "{system_prompt}<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "{user_input}<|eot_id|><|start_header_id|>assistant<|end_header_id>\n",
    "\"\"\"\n",
    "\n",
    "# System prompts\n",
    "system_prompt_idiom_en = \"\"\"\n",
    "You are an expert with deep knowledge of idioms. Your role is to provide the user with an accurate and detailed explanation of the idiom.\n",
    "\"\"\"\n",
    "system_prompt_guess_idiom_en = \"\"\"\n",
    "You are an expert with deep knowledge of idioms. Your role is to provide the user with the correct idiom.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "data_path = \"/data/uijih/previous/Seed50_for_Parallel_Dataset_ENKR_idiomKB_0.8_example.csv\"\n",
    "data = pd.read_csv(data_path)\n",
    "\n",
    "# Extract relevant columns\n",
    "idioms_en = data['Idiom'].tolist()        # English idioms\n",
    "meanings_en = data['Meaning'].tolist()    # English meanings\n",
    "idioms_kr = data['KR_Idiom'].tolist()     # Korean idioms\n",
    "meanings_kr = data['KR_Meaning'].tolist() # Korean meanings\n",
    "\n",
    "# Initialize result lists\n",
    "result_idiom_to_meaning = []\n",
    "result_meaning_to_idiom = []\n",
    "\n",
    "# Inference for idiom to meaning and vice versa\n",
    "for idiom_en, meaning_en, idiom_kr, meaning_kr in zip(idioms_en, meanings_en, idioms_kr, meanings_kr):\n",
    "    # 1. Idiom to Meaning (English)\n",
    "    system_prompt = system_prompt_idiom_en\n",
    "    user_input = f'What does the idiom \"{idiom_en}\" mean?'\n",
    "    \n",
    "    # Create prompt\n",
    "    input_prompt = prompt_template.format(system_prompt=system_prompt, user_input=user_input)\n",
    "    inputs = tokenizer(input_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, max_new_tokens=150, num_beams=5, early_stopping=True, repetition_penalty=1.1)\n",
    "    \n",
    "    decoded_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Extract response after 'assistant<|end_header_id>'\n",
    "    assistant_marker = \"assistant<|end_header_id>\"\n",
    "    assistant_response = decoded_output.split(assistant_marker, 1)[1].strip() if assistant_marker in decoded_output else decoded_output.strip()\n",
    "    \n",
    "    # Save result\n",
    "    result_idiom_to_meaning.append({\n",
    "        \"Idiom\": idiom_en,\n",
    "        \"Generated Meaning\": assistant_response,\n",
    "        \"Label\": meaning_en\n",
    "    })\n",
    "    \n",
    "    # 2. Meaning to Idiom (English)\n",
    "    system_prompt = system_prompt_guess_idiom_en\n",
    "    user_input = f'What is the idiom that means \"{meaning_en}\"?'\n",
    "    \n",
    "    # Create prompt\n",
    "    input_prompt = prompt_template.format(system_prompt=system_prompt, user_input=user_input)\n",
    "    inputs = tokenizer(input_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, max_new_tokens=150, num_beams=5, early_stopping=True, repetition_penalty=1.1)\n",
    "    \n",
    "    decoded_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Extract response\n",
    "    assistant_response = decoded_output.split(assistant_marker, 1)[1].strip() if assistant_marker in decoded_output else decoded_output.strip()\n",
    "    \n",
    "    # Save result\n",
    "    result_meaning_to_idiom.append({\n",
    "        \"Meaning\": meaning_en,\n",
    "        \"Generated Idiom\": assistant_response,\n",
    "        \"Label\": idiom_en\n",
    "    })\n",
    "    \n",
    "    # 3. Idiom to Meaning (Korean)    \n",
    "    # Create prompt\n",
    "    input_prompt = prompt_template.format(system_prompt=system_prompt, user_input=user_input)\n",
    "    inputs = tokenizer(input_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, max_new_tokens=150, num_beams=5, early_stopping=True, repetition_penalty=1.1)\n",
    "    \n",
    "    decoded_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Extract response\n",
    "    assistant_response = decoded_output.split(assistant_marker, 1)[1].strip() if assistant_marker in decoded_output else decoded_output.strip()\n",
    "    \n",
    "    # Save result\n",
    "    result_idiom_to_meaning.append({\n",
    "        \"KR_Idiom\": idiom_kr,\n",
    "        \"Generated KR Meaning\": assistant_response,\n",
    "        \"Label\": meaning_kr\n",
    "    })\n",
    "    \n",
    "    # 4. Meaning to Idiom (Korean)    \n",
    "    # Create prompt\n",
    "    input_prompt = prompt_template.format(system_prompt=system_prompt, user_input=user_input)\n",
    "    inputs = tokenizer(input_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, max_new_tokens=150, num_beams=5, early_stopping=True, repetition_penalty=1.1)\n",
    "    \n",
    "    decoded_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Extract response\n",
    "    assistant_response = decoded_output.split(assistant_marker, 1)[1].strip() if assistant_marker in decoded_output else decoded_output.strip()\n",
    "    \n",
    "    # Save result\n",
    "    result_meaning_to_idiom.append({\n",
    "        \"KR_Meaning\": meaning_kr,\n",
    "        \"Generated KR Idiom\": assistant_response,\n",
    "        \"Label\": idiom_kr\n",
    "    })\n",
    "\n",
    "# Save results to CSV\n",
    "df_idiom_to_meaning = pd.DataFrame(result_idiom_to_meaning)\n",
    "df_meaning_to_idiom = pd.DataFrame(result_meaning_to_idiom)\n",
    "\n",
    "df_idiom_to_meaning.to_csv(\"idiom_to_meaning_results.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "df_meaning_to_idiom.to_csv(\"meaning_to_idiom_results.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(\"Inference completed and results saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 그냥 translation 하게하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# 모델과 토크나이저 로드\n",
    "model_path = \"./saveded_instruct-full-1\" \n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, local_files_only=True, device_map=\"auto\")\n",
    "model.eval()\n",
    "\n",
    "# 데이터 로드\n",
    "data_path = \"/data/uijih/previous/Seed50_for_Parallel_Dataset_ENKR_idiomKB_0.8_example.csv\"\n",
    "data = pd.read_csv(data_path)\n",
    "df = pd.DataFrame(data)\n",
    "sampled_df = df\n",
    "\n",
    "# 필요한 열을 리스트로 저장\n",
    "en_sentences = sampled_df['Sentence'].tolist()  # 영어 문장 (예문)\n",
    "kr_sentences = sampled_df['KR_Sentence'].tolist() # 한국어 문장 (예문)\n",
    "kr_idioms = sampled_df['KR_Idiom'].tolist()     # 한국어 관용구\n",
    "kr_meanings = sampled_df['KR_Meaning'].tolist() # 한국어 관용구 의미\n",
    "en_idioms = sampled_df['Idiom'].tolist()        # 영어 관용구\n",
    "\n",
    "# 프롬프트 템플릿 사용\n",
    "prompt_template = \"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "{system_prompt}<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "{user_input}<|eot_id|><|start_header_id|>assistant<|end_header_id>\n",
    "\"\"\"\n",
    "\n",
    "# 시스템 프롬프트 정의\n",
    "system_prompt_translate_kr_to_en = \"You are a professional translator proficient in Korean and English. Please translate the given Korean sentence into English accurately.\"\n",
    "system_prompt_translate_en_to_kr = \"You are a professional translator proficient in Korean and English. Please translate the given English sentence into Korean accurately.\"\n",
    "\n",
    "# 결과 저장 리스트 초기화\n",
    "result_kr_to_en = []\n",
    "result_en_to_kr = []\n",
    "\n",
    "# Inference 실행\n",
    "for en_sentence, kr_sentence, kr_idiom, en_idiom in zip(en_sentences, kr_sentences, kr_idioms, en_idioms):\n",
    "    # 1. 영어 문장을 한국어로 번역 (en_sentence -> kr)\n",
    "    system_prompt = system_prompt_translate_en_to_kr\n",
    "    user_input = f'Translate the sentence \"{en_sentence}\" into Korean.'\n",
    "    \n",
    "    # 프롬프트 생성\n",
    "    input_prompt = prompt_template.format(system_prompt=system_prompt, user_input=user_input)\n",
    "    inputs = tokenizer(input_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, max_new_tokens=150, num_beams=5, early_stopping=True, repetition_penalty=1.1)\n",
    "    \n",
    "    decoded_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # 'assistant<|end_header_id>' 이후의 텍스트 추출\n",
    "    assistant_marker = \"assistant<|end_header_id>\"\n",
    "    if assistant_marker in decoded_output:\n",
    "        assistant_response = decoded_output.split(assistant_marker, 1)[1].strip()\n",
    "    else:\n",
    "        assistant_response = decoded_output.strip()\n",
    "    \n",
    "    result_en_to_kr.append({\n",
    "        \"Source EN Sentence\": en_sentence,\n",
    "        \"Generated KR Translation\": assistant_response,\n",
    "        \"label\": kr_idiom\n",
    "    })\n",
    "    # 출력\n",
    "    print(f\"EN Sentence: {en_sentence}\")\n",
    "    print(f\"KR Translation: {assistant_response}\")\n",
    "    print(f\"label: {kr_idiom}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    # 2. 한국어 문장을 영어로 번역 (kr_sentence -> en)\n",
    "    system_prompt = system_prompt_translate_kr_to_en\n",
    "    user_input = f'Translate the sentence \"{kr_sentence}\" into English.'\n",
    "    \n",
    "    # 프롬프트 생성\n",
    "    input_prompt = prompt_template.format(system_prompt=system_prompt, user_input=user_input)\n",
    "    inputs = tokenizer(input_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, max_new_tokens=150, num_beams=5, early_stopping=True, repetition_penalty=1.1)\n",
    "    \n",
    "    decoded_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # 'assistant<|end_header_id>' 이후의 텍스트 추출\n",
    "    if assistant_marker in decoded_output:\n",
    "        assistant_response = decoded_output.split(assistant_marker, 1)[1].strip()\n",
    "    else:\n",
    "        assistant_response = decoded_output.strip()\n",
    "    \n",
    "    result_kr_to_en.append({\n",
    "        \"Source KR Sentence\": kr_sentence,\n",
    "        \"Generated EN Translation\": assistant_response,\n",
    "        \"label\": en_idiom\n",
    "    })\n",
    "    # 출력\n",
    "    print(f\"KR Sentence: {kr_sentence}\")\n",
    "    print(f\"EN Translation: {assistant_response}\")\n",
    "    print(f\"label: {en_idiom}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "# 3. 결과를 CSV 파일로 저장\n",
    "df_kr_to_en = pd.DataFrame(result_kr_to_en)\n",
    "df_en_to_kr = pd.DataFrame(result_en_to_kr)\n",
    "\n",
    "df_kr_to_en.to_csv(\"kr_sentence_to_en_translation.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "df_en_to_kr.to_csv(\"en_sentence_to_kr_translation.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(\"CSV files have been saved successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# instruct 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# 데이터 로드\n",
    "data_path = \"/data/uijih/previous/Seed50_for_Parallel_Dataset_ENKR_idiomKB_0.8_example.csv\"\n",
    "data = pd.read_csv(data_path)\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "sampled_df = df\n",
    "\n",
    "# 필요한 열을 리스트로 저장\n",
    "en_sentences = sampled_df['Sentence'].tolist()  # 영어 문장 (예문)\n",
    "kr_sentences = sampled_df['KR_Sentence'].tolist() # 한국어 문장 (예문)\n",
    "kr_idioms = sampled_df['KR_Idiom'].tolist()     # 한국어 관용구\n",
    "kr_meanings = sampled_df['KR_Meaning'].tolist() # 한국어 관용구 의미\n",
    "en_idioms = sampled_df['Idiom'].tolist()        # 영어 관용구\n",
    "\n",
    "# 프롬프트 템플릿 사용\n",
    "prompt_template = \"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "{system_prompt}<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "{user_input}<|eot_id|><|start_header_id|>assistant<|end_header_id>\n",
    "\"\"\"\n",
    "# 필요한 열을 리스트로 저장\n",
    "en_sentences = sampled_df['Sentence'].tolist()  # 영어 문장 (예문)\n",
    "kr_sentences = sampled_df['KR_Sentence'].tolist() \n",
    "kr_idioms = sampled_df['KR_Idiom'].tolist()     # 한국어 관용구\n",
    "kr_meanings = sampled_df['KR_Meaning'].tolist() # 한국어 관용구 의미\n",
    "en_idioms = sampled_df['Idiom'].tolist()        # 영어 관용구\n",
    "\n",
    "# 프롬프트 템플릿 사용\n",
    "prompt_template = \"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "{system_prompt}<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "{user_input}<|eot_id|><|start_header_id|>assistant<|end_header_id>\n",
    "\"\"\"\n",
    "\n",
    "# 시스템 프롬프트 정의\n",
    "system_prompt_translate_kr_to_en = \"\"\"\n",
    "You are a professional idiom translator proficient in Korean Idioms and English Idioms. Please translate the given Korean sentence into English accurately, ensuring that any Korean idiom is translated into its correct English idiom equivalent. Non-idiomatic parts of the sentence should be translated naturally and fluently.\n",
    "\"\"\"\n",
    "system_prompt_translate_en_to_kr = \"\"\"\n",
    "You are a professional idiom translator proficient in English Idioms and Korean Idioms. Please translate the given English sentence into Korean accurately, ensuring that any English idiom is translated into its correct Korean idiom equivalent. Non-idiomatic parts of the sentence should be translated naturally and fluently.\n",
    "\"\"\"\n",
    "\n",
    "# 결과 저장 리스트 초기화\n",
    "result_kr_to_en = []\n",
    "result_en_to_kr = []\n",
    "\n",
    "# Inference 실행\n",
    "for en_sentence, kr_sentence, kr_idiom, en_idiom in zip(en_sentences, kr_sentences, kr_idioms, en_idioms):\n",
    "    # 1. 영어 문장을 한국어로 번역 (en_sentence -> kr)\n",
    "    system_prompt = system_prompt_translate_en_to_kr\n",
    "    user_input = f'Translate the sentence \"{en_sentence}\" into Korean.'\n",
    "    \n",
    "    # 프롬프트 생성\n",
    "    input_prompt = prompt_template.format(system_prompt=system_prompt, user_input=user_input)\n",
    "    inputs = tokenizer(input_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, max_new_tokens=150, num_beams=5, early_stopping=True, repetition_penalty=1.1)\n",
    "    \n",
    "    decoded_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # 'assistant<|end_header_id>' 이후의 텍스트 추출\n",
    "    assistant_marker = \"assistant<|end_header_id>\"\n",
    "    if assistant_marker in decoded_output:\n",
    "        assistant_response = decoded_output.split(assistant_marker, 1)[1].strip()\n",
    "    else:\n",
    "        assistant_response = decoded_output.strip()\n",
    "    \n",
    "    result_en_to_kr.append({\n",
    "        \"Source EN Sentence\": en_sentence,\n",
    "        \"Generated KR Translation\": assistant_response,\n",
    "        \"label\": kr_idiom\n",
    "    })\n",
    "    # 출력\n",
    "    print(f\"EN Sentence: {en_sentence}\")\n",
    "    print(f\"KR Translation: {assistant_response}\")\n",
    "    print(f\"label: {kr_idiom}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    # 2. 한국어 문장을 영어로 번역 (kr_sentence -> en)\n",
    "    system_prompt = system_prompt_translate_kr_to_en\n",
    "    user_input = f'Translate the sentence \"{kr_sentence}\" into English.'\n",
    "    \n",
    "    # 프롬프트 생성\n",
    "    input_prompt = prompt_template.format(system_prompt=system_prompt, user_input=user_input)\n",
    "    inputs = tokenizer(input_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, max_new_tokens=150, num_beams=5, early_stopping=True, repetition_penalty=1.1)\n",
    "    \n",
    "    decoded_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # 'assistant<|end_header_id>' 이후의 텍스트 추출\n",
    "    if assistant_marker in decoded_output:\n",
    "        assistant_response = decoded_output.split(assistant_marker, 1)[1].strip()\n",
    "    else:\n",
    "        assistant_response = decoded_output.strip()\n",
    "    \n",
    "    result_kr_to_en.append({\n",
    "        \"Source KR Sentence\": kr_sentence,\n",
    "        \"Generated EN Translation\": assistant_response,\n",
    "        \"label\": en_idiom\n",
    "    })\n",
    "    # 출력\n",
    "    print(f\"KR Sentence: {kr_sentence}\")\n",
    "    print(f\"EN Translation: {assistant_response}\")\n",
    "    print(f\"label: {en_idiom}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "# 3. 결과를 CSV 파일로 저장\n",
    "df_kr_to_en = pd.DataFrame(result_kr_to_en)\n",
    "df_en_to_kr = pd.DataFrame(result_en_to_kr)\n",
    "\n",
    "df_kr_to_en.to_csv(\"kr_sentence_to_en_translation_i.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "df_en_to_kr.to_csv(\"en_sentence_to_kr_translation_i.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(\"CSV files have been saved successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# shot 주고"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 로드\n",
    "data_path = '/data/uijih/previous/Seed50_for_Parallel_Dataset_ENKR_idiomKB_0.8_example.csv'\n",
    "data = pd.read_csv(data_path)\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# 'Sentence' 열에서 NaN 값 제거\n",
    "df = df.dropna(subset=['Sentence'])\n",
    "\n",
    "sampled_df = df.head(25)  # 25개의 샘플을 사용\n",
    "\n",
    "# 필요한 열을 리스트로 저장\n",
    "en_sentences = sampled_df['Sentence'].tolist()  # 영어 문장 (예문)\n",
    "kr_sentences = sampled_df['KR_Sentence'].tolist() \n",
    "kr_idioms = sampled_df['KR_Idiom'].tolist()     # 한국어 관용구\n",
    "kr_meanings = sampled_df['KR_Meaning'].tolist() # 한국어 관용구 의미\n",
    "en_idioms = sampled_df['Idiom'].tolist()        # 영어 관용구\n",
    "\n",
    "# 프롬프트 템플릿 사용\n",
    "prompt_template = \"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "{system_prompt}<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "{user_input}<|eot_id|><|start_header_id|>assistant<|end_header_id>\n",
    "\"\"\"\n",
    "\n",
    "# 시스템 프롬프트 정의\n",
    "system_prompt_translate_kr_to_en = \"\"\"\n",
    "You are a professional idiom translator proficient in Korean Idioms and English Idioms. Please translate the given Korean sentence into English accurately, ensuring that any Korean idiom is translated into its correct English idiom equivalent. Non-idiomatic parts of the sentence should be translated naturally and fluently.\n",
    "\n",
    "Example 1:\n",
    "Korean: \"그녀는 그녀의 시험 결과에 입이 가로 터졌다.\"\n",
    "English: \"She was over the moon with her exam results.\"\n",
    "\n",
    "Example 2:\n",
    "Korean: \"그들이 아무리 노력해봐도, 그녀는 절대 무릎 꿇지 않았다.\"\n",
    "English: \"No matter how hard they tried, she wouldn't say uncle.\"\n",
    "\n",
    "Now, translate the following sentence.\n",
    "\"\"\"\n",
    "\n",
    "system_prompt_translate_en_to_kr = \"\"\"\n",
    "You are a professional idiom translator proficient in English Idioms and Korean Idioms. Please translate the given English sentence into Korean accurately, ensuring that any English idiom is translated into its correct Korean idiom equivalent. Non-idiomatic parts of the sentence should be translated naturally and fluently.\n",
    "\n",
    "Example 1:\n",
    "English: \"She was over the moon with her exam results.\"\n",
    "Korean: \"그녀는 그녀의 시험 결과에 입이 가로 터졌다.\"\n",
    "\n",
    "Example 2:\n",
    "English: \"No matter how hard they tried, she wouldn't say uncle.\"\n",
    "Korean: \"그들이 아무리 노력해봐도, 그녀는 절대 무릎 꿇지 않았다.\"\n",
    "\n",
    "Now, translate the following sentence.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# 결과 저장 리스트 초기화\n",
    "result_kr_to_en = []\n",
    "result_en_to_kr = []\n",
    "\n",
    "# Inference 실행\n",
    "for en_sentence, kr_sentence, kr_idiom, en_idiom in zip(en_sentences, kr_sentences, kr_idioms, en_idioms):\n",
    "    # 1. 영어 문장을 한국어로 번역 (en_sentence -> kr)\n",
    "    system_prompt = system_prompt_translate_en_to_kr\n",
    "    user_input = f'Translate the sentence \"{en_sentence}\" into Korean.'\n",
    "    \n",
    "    # 프롬프트 생성\n",
    "    input_prompt = prompt_template.format(system_prompt=system_prompt, user_input=user_input)\n",
    "    inputs = tokenizer(input_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, max_new_tokens=150, num_beams=5, early_stopping=True, repetition_penalty=1.1)\n",
    "    \n",
    "    decoded_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # 'assistant<|end_header_id>' 이후의 텍스트 추출\n",
    "    assistant_marker = \"assistant<|end_header_id>\"\n",
    "    if assistant_marker in decoded_output:\n",
    "        assistant_response = decoded_output.split(assistant_marker, 1)[1].strip()\n",
    "    else:\n",
    "        assistant_response = decoded_output.strip()\n",
    "    \n",
    "    result_en_to_kr.append({\n",
    "        \"Source EN Sentence\": en_sentence,\n",
    "        \"Generated KR Translation\": assistant_response,\n",
    "        \"label\": kr_idiom\n",
    "    })\n",
    "    # 출력\n",
    "    print(f\"EN Sentence: {en_sentence}\")\n",
    "    print(f\"KR Translation: {assistant_response}\")\n",
    "    print(f\"label: {kr_idiom}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    # 2. 한국어 문장을 영어로 번역 (kr_sentence -> en)\n",
    "    system_prompt = system_prompt_translate_kr_to_en\n",
    "    user_input = f'Translate the sentence \"{kr_sentence}\" into English.'\n",
    "    \n",
    "    # 프롬프트 생성\n",
    "    input_prompt = prompt_template.format(system_prompt=system_prompt, user_input=user_input)\n",
    "    inputs = tokenizer(input_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, max_new_tokens=150, num_beams=5, early_stopping=True, repetition_penalty=1.1)\n",
    "    \n",
    "    decoded_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # 'assistant<|end_header_id>' 이후의 텍스트 추출\n",
    "    if assistant_marker in decoded_output:\n",
    "        assistant_response = decoded_output.split(assistant_marker, 1)[1].strip()\n",
    "    else:\n",
    "        assistant_response = decoded_output.strip()\n",
    "    \n",
    "    result_kr_to_en.append({\n",
    "        \"Source KR Sentence\": kr_sentence,\n",
    "        \"Generated EN Translation\": assistant_response,\n",
    "        \"label\": en_idiom\n",
    "    })\n",
    "    # 출력\n",
    "    print(f\"KR Sentence: {kr_sentence}\")\n",
    "    print(f\"EN Translation: {assistant_response}\")\n",
    "    print(f\"label: {en_idiom}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "# 3. 결과를 CSV 파일로 저장\n",
    "df_kr_to_en = pd.DataFrame(result_kr_to_en)\n",
    "df_en_to_kr = pd.DataFrame(result_en_to_kr)\n",
    "\n",
    "df_kr_to_en.to_csv(\"kr_sentence_to_en_translation_p.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "df_en_to_kr.to_csv(\"en_sentence_to_kr_translation_p.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(\"CSV files have been saved successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# 모델과 토크나이저 로드\n",
    "#model_path = \"/data/uijih/instruct/llama3.1-8b-Instruct-full/checkpoint-385\"\n",
    "\n",
    "#tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "#model = AutoModelForCausalLM.from_pretrained(model_path, local_files_only=True, device_map=\"auto\")\n",
    "model.eval()\n",
    "\n",
    "# 특수 토큰 추가\n",
    "special_tokens_dict = {\n",
    "    'additional_special_tokens': [\n",
    "        '<|begin_of_text|>', '<|end_of_text|>', '<|finetune_right_pad_id|>',\n",
    "        '<|start_header_id|>', '<|end_header_id|>', '<|eom_id|>', '<|eot_id|>',\n",
    "        '<|python_tag|>'\n",
    "    ]\n",
    "}\n",
    "tokenizer.add_special_tokens(special_tokens_dict)\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# 데이터 로드\n",
    "data_path = '/data/uijih/previous/Seed50_for_Parallel_Dataset_ENKR_idiomKB_0.8_example.csv'\n",
    "data = pd.read_csv(data_path)\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# 'Sentence' 열에서 NaN 값 제거\n",
    "df = df.dropna(subset=['Sentence'])\n",
    "\n",
    "sampled_df = df.head(25)  # 25개의 샘플을 사용\n",
    "\n",
    "# 필요한 열을 리스트로 저장\n",
    "en_sentences = sampled_df['Sentence'].tolist()  # 영어 문장 (예문)\n",
    "kr_sentences = sampled_df['KR_Sentence'].tolist() \n",
    "kr_idioms = sampled_df['KR_Idiom'].tolist()     # 한국어 관용구\n",
    "kr_meanings = sampled_df['KR_Meaning'].tolist() # 한국어 관용구 의미\n",
    "en_idioms = sampled_df['Idiom'].tolist()        # 영어 관용구\n",
    "\n",
    "# 프롬프트 템플릿 사용\n",
    "prompt_template = \"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "{system_prompt}<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "{user_input}<|eot_id|><|start_header_id|>assistant<|end_header_id>\n",
    "\"\"\"\n",
    "\n",
    "# 시스템 프롬프트 정의\n",
    "system_prompt_translate_en_to_kr_cot = \"\"\"\n",
    "You are a professional idiom translator proficient in English Idioms and Korean Idioms. Follow a step-by-step process to translate the given English sentence into Korean, ensuring that any English idiom is accurately translated into its corresponding Korean idiom equivalent. Translate non-idiomatic parts naturally and fluently.\n",
    "\n",
    "Step 1: Detect the idiom in the given sentence.\n",
    "Step 2: Explain the meaning of the detected idiom in English.\n",
    "Step 3: Translate the meaning into Korean.\n",
    "Step 4: Find a Korean idiom that conveys the same meaning.\n",
    "Step 5: Translate the entire sentence into Korean, using the identified Korean idiom.\n",
    "\n",
    "Example 1:\n",
    "English: \"She was over the moon with her exam results.\"\n",
    "Step 1: Detected idiom: \"over the moon\"\n",
    "Step 2: Meaning: \"extremely happy or delighted\"\n",
    "Step 3: Translated meaning: \"매우 행복하거나 기쁘다\"\n",
    "Step 4: Korean idiom: \"입이 가로 터지다\"\n",
    "Step 5: Translated sentence: \"그녀는 그녀의 시험 결과에 입이 가로 터졌다.\"\n",
    "\n",
    "Example 2:\n",
    "English: \"No matter how hard they tried, she wouldn't say uncle.\"\n",
    "Step 1: Detected idiom: \"say uncle\"\n",
    "Step 2: Meaning: \"to admit defeat or surrender\"\n",
    "Step 3: Translated meaning: \"패배를 인정하거나 항복하다\"\n",
    "Step 4: Korean idiom: \"무릎을 꿇다\"\n",
    "Step 5: Translated sentence: \"그들이 아무리 노력해봐도, 그녀는 절대 무릎 꿇지 않았다.\"\n",
    "\n",
    "Now, follow the steps to translate the following sentence.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "system_prompt_translate_kr_to_en_cot = \"\"\"\n",
    "You are a professional idiom translator proficient in Korean Idioms and English Idioms. Follow a step-by-step process to translate the given Korean sentence into English, ensuring that any Korean idiom is accurately translated into its corresponding English idiom equivalent. Translate non-idiomatic parts naturally and fluently.\n",
    "\n",
    "Step 1: Detect the idiom in the given sentence.\n",
    "Step 2: Explain the meaning of the detected idiom in Korean.\n",
    "Step 3: Translate the meaning into English.\n",
    "Step 4: Find an English idiom that conveys the same meaning.\n",
    "Step 5: Translate the entire sentence into English, using the identified English idiom.\n",
    "\n",
    "Example 1:\n",
    "Korean: \"그녀는 그녀의 시험 결과에 입이 가로 터졌다.\"\n",
    "Step 1: Detected idiom: \"입이 가로 터지다\"\n",
    "Step 2: Meaning: \"매우 기쁘거나 행복하다\"\n",
    "Step 3: Translated meaning: \"extremely happy or delighted\"\n",
    "Step 4: English idiom: \"over the moon\"\n",
    "Step 5: Translated sentence: \"She was over the moon with her exam results.\"\n",
    "\n",
    "Example 2:\n",
    "Korean: \"그들이 아무리 노력해봐도, 그녀는 절대 무릎 꿇지 않았다.\"\n",
    "Step 1: Detected idiom: \"무릎을 꿇다\"\n",
    "Step 2: Meaning: \"패배를 인정하거나 항복하다\"\n",
    "Step 3: Translated meaning: \"to admit defeat or surrender\"\n",
    "Step 4: English idiom: \"say uncle\"\n",
    "Step 5: Translated sentence: \"No matter how hard they tried, she wouldn't say uncle.\"\n",
    "\n",
    "Now, follow the steps to translate the following sentence.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# 결과 저장 리스트 초기화\n",
    "result_kr_to_en = []\n",
    "result_en_to_kr = []\n",
    "\n",
    "# Inference 실행\n",
    "for en_sentence, kr_sentence, kr_idiom, en_idiom in zip(en_sentences, kr_sentences, kr_idioms, en_idioms):\n",
    "    # 1. 영어 문장을 한국어로 번역 (en_sentence -> kr)\n",
    "    system_prompt = system_prompt_translate_en_to_kr\n",
    "    user_input = f'Translate the sentence \"{en_sentence}\" into Korean.'\n",
    "    \n",
    "    # 프롬프트 생성\n",
    "    input_prompt = prompt_template.format(system_prompt=system_prompt, user_input=user_input)\n",
    "    inputs = tokenizer(input_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, max_new_tokens=150, num_beams=5, early_stopping=True, repetition_penalty=1.1)\n",
    "    \n",
    "    decoded_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # 'assistant<|end_header_id>' 이후의 텍스트 추출\n",
    "    assistant_marker = \"assistant<|end_header_id>\"\n",
    "    if assistant_marker in decoded_output:\n",
    "        assistant_response = decoded_output.split(assistant_marker, 1)[1].strip()\n",
    "    else:\n",
    "        assistant_response = decoded_output.strip()\n",
    "    \n",
    "    result_en_to_kr.append({\n",
    "        \"Source EN Sentence\": en_sentence,\n",
    "        \"Generated KR Translation\": assistant_response,\n",
    "        \"label\": kr_idiom\n",
    "    })\n",
    "    # 출력\n",
    "    print(f\"EN Sentence: {en_sentence}\")\n",
    "    print(f\"KR Translation: {assistant_response}\")\n",
    "    print(f\"label: {kr_idiom}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    # 2. 한국어 문장을 영어로 번역 (kr_sentence -> en)\n",
    "    system_prompt = system_prompt_translate_kr_to_en\n",
    "    user_input = f'Translate the sentence \"{kr_sentence}\" into English.'\n",
    "    \n",
    "    # 프롬프트 생성\n",
    "    input_prompt = prompt_template.format(system_prompt=system_prompt, user_input=user_input)\n",
    "    inputs = tokenizer(input_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, max_new_tokens=150, num_beams=5, early_stopping=True, repetition_penalty=1.1)\n",
    "    \n",
    "    decoded_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # 'assistant<|end_header_id>' 이후의 텍스트 추출\n",
    "    if assistant_marker in decoded_output:\n",
    "        assistant_response = decoded_output.split(assistant_marker, 1)[1].strip()\n",
    "    else:\n",
    "        assistant_response = decoded_output.strip()\n",
    "    \n",
    "    result_kr_to_en.append({\n",
    "        \"Source KR Sentence\": kr_sentence,\n",
    "        \"Generated EN Translation\": assistant_response,\n",
    "        \"label\": en_idiom\n",
    "    })\n",
    "    # 출력\n",
    "    print(f\"KR Sentence: {kr_sentence}\")\n",
    "    print(f\"EN Translation: {assistant_response}\")\n",
    "    print(f\"label: {en_idiom}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "# 3. 결과를 CSV 파일로 저장\n",
    "df_kr_to_en = pd.DataFrame(result_kr_to_en)\n",
    "df_en_to_kr = pd.DataFrame(result_en_to_kr)\n",
    "\n",
    "df_kr_to_en.to_csv(\"kr_sentence_to_en_translation_c.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "df_en_to_kr.to_csv(\"en_sentence_to_kr_translation_c.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(\"CSV files have been saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3.11.3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
