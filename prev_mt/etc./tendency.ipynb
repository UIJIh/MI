{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.99it/s]\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source: After spending every summer together since childhood, they seemed to be joined at the hip.\n",
      "Assistant (EN to KR): \n",
      "Translate the following sentence into Korean: 'After spending every summer together since childhood, they seemed to be joined at the hip.'\n",
      "This sentence contains the idiom 'be joined at the hip', which means 'to be extremely close or inseparable, often referring to two people who spend a lot of time together and do everything together'. The Korean idiom similar to 'be joined at the hip' is '구름 갈 제 비가 간다', which means'사람의 긴밀한 관계를 비유적으로 이르는 말.'.\n",
      "Translate the entire sentence by naturally replacing the English idiomatic expression with this Korean idiomatic expression. For a natural translation, feel free to change the tense or voice of the idiom if necessary.\n",
      "Here's the translation: \n",
      "\n",
      "'어린 시절부터 매년 여름을 함께 보냈던 두 사람 사이에는 서로가 너무나 잘 어울려서 한 몸처럼 되어 보였다.'\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source: After his trial, the judge sentenced him to five years behind bars for his crimes.\n",
      "Assistant (EN to KR): \n",
      "Translate the following sentence into Korean: 'After his trial, the judge sentenced him to five years behind bars for his crimes.'\n",
      "This sentence contains the idiom 'behind bars', which means 'to be in prison or jail'. The Korean idiom similar to 'behind bars' is '콩밥 먹이다', which means '감옥살이를 하게 감옥으로 보내다.'.\n",
      "Translate the entire sentence by naturally replacing the English idiomatic expression with this Korean idiomatic expression. For a natural translation, feel free to change the tense or voice of the idiom if necessary.\n",
      "Here's the translation: \n",
      "\n",
      "'그의 재판이 끝난 후, 판사는 그의 범죄에 대한 처벌로 5년을 감방에 보내라고 판결했다.'\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source: After losing that final match, they really bit the dust and couldn't make it to the championship.\n",
      "Assistant (EN to KR): \n",
      "Translate the following sentence into Korean: 'After losing that final match, they really bit the dust and couldn't make it to the championship.'\n",
      "This sentence contains the idiom 'bite the dust', which means 'to suffer defeat or failure'. The Korean idiom similar to 'bite the dust' is '쓴잔을 안기다', which means '실패나 패배 따위의 쓰라린 일을 당하게 하다.'.\n",
      "Translate the entire sentence by naturally replacing the English idiomatic expression with this Korean idiomatic expression. For a natural translation, feel free to change the tense or voice of the idiom if necessary.\n",
      "Here's the translation: \n",
      "\n",
      "'그 마지막 경기에서 패한 후, 그들은 정말 쩌린 잔을 앓게 됨을 비유적으로 이르는 말.'\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source: The detective almost blew his cover when he accidentally revealed his true profession to the suspect.\n",
      "Assistant (EN to KR): \n",
      "Translate the following sentence into Korean: 'The detective almost blew his cover when he accidentally revealed his true profession to the suspect.'\n",
      "This sentence contains the idiom 'blow someone's cover', which means 'to reveal someone's secret or hidden identity or activities'. The Korean idiom similar to 'blow someone's cover' is '뒤가 드러나다', which means '비밀로 하거나 숨긴 일이 나타나거나 알려지다.'.\n",
      "Translate the entire sentence by naturally replacing the English idiomatic expression with this Korean idiomatic expression. For a natural translation, feel free to change the tense or voice of the idiom if necessary.\n",
      "Here's the translation: \n",
      "\n",
      "'탐정은 의심받지 않으려는 사이에 범인에게 자신의 진정한 직업이 무엇인지 실수로 알려 버렸다.'\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source: The investigative journalist managed to blow the lid off the corruption scandal, shocking the entire community.\n",
      "Assistant (EN to KR): \n",
      "Translate the following sentence into Korean: 'The investigative journalist managed to blow the lid off the corruption scandal, shocking the entire community.'\n",
      "This sentence contains the idiom 'blow the lid off', which means 'to reveal or expose something secret or scandalous'. The Korean idiom similar to 'blow the lid off' is '뒤가 드러나다', which means '비밀로 하거나 숨긴 일이 나타나거나 알려지다.'.\n",
      "Translate the entire sentence by naturally replacing the English idiomatic expression with this Korean idiomatic expression. For a natural translation, feel free to change the tense or voice of the idiom if necessary.\n",
      "Here's the translation: \n",
      "\n",
      "'수사 저널리스트은 온 사회를 충격에 빠뜨리는 부패 스캔들을 폭로함을 이끌어 냈다.'\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source: The investigation aimed to break open the corruption scandal that had plagued the city for years.\n",
      "Assistant (EN to KR): \n",
      "Translate the following sentence into Korean: 'The investigation aimed to break open the corruption scandal that had plagued the city for years.'\n",
      "This sentence contains the idiom 'break open', which means 'to reveal or expose something that was previously hidden or secret'. The Korean idiom similar to 'break open' is '뒤가 드러나다', which means '비밀로 하거나 숨긴 일이 나타나거나 알려지다.'.\n",
      "Translate the entire sentence by naturally replacing the English idiomatic expression with this Korean idiomatic expression. For a natural translation, feel free to change the tense or voice of the idiom if necessary.\n",
      "Here's the translation: \n",
      "\n",
      "'이 조사 목적은 여러 해 동안 도시를 괴롭혔던 부패 스캔들을 밝히기 위함이었다.'\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source: In the final minutes of the game, the coach decided to go for broke and put their best players on the field.\n",
      "Assistant (EN to KR): \n",
      "Translate the following sentence into Korean: 'In the final minutes of the game, the coach decided to go for broke and put their best players on the field.'\n",
      "This sentence contains the idiom 'go for broke', which means 'to take a big risk, to put everything on the line, to go all out in pursuit of something; to give maximum effort or commitment'. The Korean idiom similar to 'go for broke' is '삼수갑산에 가는 한이 있어도', which means '자신에게 닥쳐올 어떤 위험도 무릅쓰고라도 어떤 일을 단행할 때 하는 말.'.\n",
      "Translate the entire sentence by naturally replacing the English idiomatic expression with this Korean idiomatic expression. For a natural translation, feel free to change the tense or voice of the idiom if necessary.\n",
      "Here's the translation: \n",
      "\n",
      "'게임의 마지막 분위기에서, 코치가 최선을 다하는 선수들을 경기장에 내보내는 결정을 내렸다.'\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source: After years of mismanagement, the once-thriving company eventually went to the wall.\n",
      "Assistant (EN to KR): \n",
      "Translate the following sentence into Korean: 'After years of mismanagement, the once-thriving company eventually went to the wall.'\n",
      "This sentence contains the idiom 'go to the wall', which means 'to face failure or ruin; to be defeated or destroyed'. The Korean idiom similar to 'go to the wall' is '쓴잔을 안기다', which means '실패나 패배 따위의 쓰라린 일을 당하게 하다.'.\n",
      "Translate the entire sentence by naturally replacing the English idiomatic expression with this Korean idiomatic expression. For a natural translation, feel free to change the tense or voice of the idiom if necessary.\n",
      "Here's the translated sentence: \n",
      "\n",
      "'몇 년 동안 잘못된 경영으로 번성하던 회사도 결국 쇠퇴하여 망하게 되었다.'\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source: All our plans for the weekend went up in smoke when the storm hit the area.\n",
      "Assistant (EN to KR): \n",
      "Translate the following sentence into Korean: 'All our plans for the weekend went up in smoke when the storm hit the area.'\n",
      "This sentence contains the idiom 'go up in smoke', which means 'to be completely destroyed or ruined; to fail utterly or come to nothing'. The Korean idiom similar to 'go up in smoke' is '닭 쫓던 개 울타리 넘겨다보듯', which means'애써 하던 일이 실패로 돌아가거나 남보다 뒤떨어져 어찌할 도리가 없이 됨을 비유적으로 이르는 말.'.\n",
      "Translate the entire sentence by naturally replacing the English idiomatic expression with this Korean idiomatic expression. For a natural translation, feel free to change the tense or voice of the idiom if necessary.\n",
      "Here's the translation: \n",
      "\n",
      "'주말에 계획했던 모든 일들이 폭우로 인해 망가져 버렸습니다.'\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source: After the merger, the company’s profits started to head south, prompting a complete overhaul of their strategy.\n",
      "Assistant (EN to KR): \n",
      "Translate the following sentence into Korean: 'After the merger, the company’s profits started to head south, prompting a complete overhaul of their strategy.'\n",
      "This sentence contains the idiom 'head south', which means 'to deteriorate or go wrong; to decline or become unsuccessful'. The Korean idiom similar to 'head south' is '닭 쫓던 개 울타리 넘겨다보듯', which means'애써 하던 일이 실패로 돌아가거나 남보다 뒤떨어져 어찌할 도리가 없이 됨을 비유적으로 이르는 말.'.\n",
      "Translate the entire sentence by naturally replacing the English idiomatic expression with this Korean idiomatic expression. For a natural translation, feel free to change the tense or voice of the idiom if necessary.\n",
      "Here's the translation: \n",
      "\n",
      "'합병 이후 회사의 이익은 급격히 떨어지기 시작했고, 이에 따라 그들의 전략을 완전히 바꿔야 함을 뜻합니다.'\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 70\u001b[0m\n\u001b[1;32m     67\u001b[0m final_prompt_en_to_kr \u001b[38;5;241m=\u001b[39m create_prompt(item)\n\u001b[1;32m     69\u001b[0m \u001b[38;5;66;03m# Generate model responses\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m translation_response_en_to_kr \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfinal_prompt_en_to_kr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;66;03m# Extract the translation by removing the prompt\u001b[39;00m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;66;03m#en_to_kr_translation = translation_response_en_to_kr[len(final_prompt_en_to_kr):].strip()\u001b[39;00m\n\u001b[1;32m     74\u001b[0m en_to_kr_translation \u001b[38;5;241m=\u001b[39m translation_response_en_to_kr\n",
      "Cell \u001b[0;32mIn[10], line 29\u001b[0m, in \u001b[0;36mgenerate_response\u001b[0;34m(user_input, max_length)\u001b[0m\n\u001b[1;32m     20\u001b[0m inputs \u001b[38;5;241m=\u001b[39m tokenizer(\n\u001b[1;32m     21\u001b[0m     user_input,\n\u001b[1;32m     22\u001b[0m     return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     25\u001b[0m     max_length\u001b[38;5;241m=\u001b[39mmax_length\n\u001b[1;32m     26\u001b[0m )\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Generate response from the model\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_beams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43mno_repeat_ngram_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\n\u001b[1;32m     35\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# Decode the response\u001b[39;00m\n\u001b[1;32m     38\u001b[0m response \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mdecode(outputs[\u001b[38;5;241m0\u001b[39m], skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/envs/python3.11.3/lib/python3.11/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/python3.11.3/lib/python3.11/site-packages/transformers/generation/utils.py:2063\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   2055\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2056\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2057\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[1;32m   2058\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2059\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2060\u001b[0m     )\n\u001b[1;32m   2062\u001b[0m     \u001b[38;5;66;03m# 14. run beam sample\u001b[39;00m\n\u001b[0;32m-> 2063\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_beam_search\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2064\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2065\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeam_scorer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2066\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2067\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_warper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_warper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2068\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2069\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2070\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2071\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2072\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2074\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mGROUP_BEAM_SEARCH:\n\u001b[1;32m   2075\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[1;32m   2076\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[1;32m   2077\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   2078\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2084\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[1;32m   2085\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/python3.11.3/lib/python3.11/site-packages/transformers/generation/utils.py:3251\u001b[0m, in \u001b[0;36mGenerationMixin._beam_search\u001b[0;34m(self, input_ids, beam_scorer, logits_processor, stopping_criteria, generation_config, synced_gpus, logits_warper, **model_kwargs)\u001b[0m\n\u001b[1;32m   3246\u001b[0m next_token_logits \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlogits[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :]\u001b[38;5;241m.\u001b[39mclone()\n\u001b[1;32m   3247\u001b[0m next_token_scores \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mlog_softmax(\n\u001b[1;32m   3248\u001b[0m     next_token_logits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   3249\u001b[0m )  \u001b[38;5;66;03m# (batch_size * num_beams, vocab_size)\u001b[39;00m\n\u001b[0;32m-> 3251\u001b[0m next_token_scores_processed \u001b[38;5;241m=\u001b[39m \u001b[43mlogits_processor\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnext_token_scores\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3252\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m do_sample:\n\u001b[1;32m   3253\u001b[0m     next_token_scores_processed \u001b[38;5;241m=\u001b[39m logits_warper(input_ids, next_token_scores_processed)\n",
      "File \u001b[0;32m~/anaconda3/envs/python3.11.3/lib/python3.11/site-packages/transformers/generation/logits_process.py:98\u001b[0m, in \u001b[0;36mLogitsProcessorList.__call__\u001b[0;34m(self, input_ids, scores, **kwargs)\u001b[0m\n\u001b[1;32m     96\u001b[0m         scores \u001b[38;5;241m=\u001b[39m processor(input_ids, scores, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 98\u001b[0m         scores \u001b[38;5;241m=\u001b[39m \u001b[43mprocessor\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscores\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m scores\n",
      "File \u001b[0;32m~/anaconda3/envs/python3.11.3/lib/python3.11/site-packages/transformers/generation/logits_process.py:968\u001b[0m, in \u001b[0;36mNoRepeatNGramLogitsProcessor.__call__\u001b[0;34m(self, input_ids, scores)\u001b[0m\n\u001b[1;32m    966\u001b[0m cur_len \u001b[38;5;241m=\u001b[39m input_ids\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    967\u001b[0m scores_processed \u001b[38;5;241m=\u001b[39m scores\u001b[38;5;241m.\u001b[39mclone()\n\u001b[0;32m--> 968\u001b[0m banned_batch_tokens \u001b[38;5;241m=\u001b[39m \u001b[43m_calc_banned_ngram_tokens\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mngram_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_batch_hypotheses\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcur_len\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    969\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, banned_tokens \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(banned_batch_tokens):\n\u001b[1;32m    970\u001b[0m     scores_processed[i, banned_tokens] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minf\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/python3.11.3/lib/python3.11/site-packages/transformers/generation/logits_process.py:908\u001b[0m, in \u001b[0;36m_calc_banned_ngram_tokens\u001b[0;34m(ngram_size, prev_input_ids, num_hypos, cur_len)\u001b[0m\n\u001b[1;32m    905\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cur_len \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m<\u001b[39m ngram_size:\n\u001b[1;32m    906\u001b[0m     \u001b[38;5;66;03m# return no banned tokens if we haven't generated no_repeat_ngram_size tokens yet\u001b[39;00m\n\u001b[1;32m    907\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [[] \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_hypos)]\n\u001b[0;32m--> 908\u001b[0m generated_ngrams \u001b[38;5;241m=\u001b[39m \u001b[43m_get_ngrams\u001b[49m\u001b[43m(\u001b[49m\u001b[43mngram_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprev_input_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_hypos\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    909\u001b[0m banned_tokens \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    910\u001b[0m     _get_generated_ngrams(generated_ngrams[hypo_idx], prev_input_ids[hypo_idx], ngram_size, cur_len)\n\u001b[1;32m    911\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hypo_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_hypos)\n\u001b[1;32m    912\u001b[0m ]\n\u001b[1;32m    913\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m banned_tokens\n",
      "File \u001b[0;32m~/anaconda3/envs/python3.11.3/lib/python3.11/site-packages/transformers/generation/logits_process.py:869\u001b[0m, in \u001b[0;36m_get_ngrams\u001b[0;34m(ngram_size, prev_input_ids, num_hypos)\u001b[0m\n\u001b[1;32m    867\u001b[0m generated_ngrams \u001b[38;5;241m=\u001b[39m [{} \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_hypos)]\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_hypos):\n\u001b[0;32m--> 869\u001b[0m     gen_tokens \u001b[38;5;241m=\u001b[39m \u001b[43mprev_input_ids\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtolist\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    870\u001b[0m     generated_ngram \u001b[38;5;241m=\u001b[39m generated_ngrams[idx]\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;66;03m# Loop through each n-gram of size ngram_size in the list of tokens (gen_tokens)\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Fine-Tuned model path\n",
    "model_output_dir = \"/data/uijih/8b_instruct/model_output/llama3_sft_idioms-full-m\"\n",
    "\n",
    "# Load the tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_output_dir, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_output_dir, torch_dtype=torch.bfloat16).to('cuda')\n",
    "# base\n",
    "tokenizer.add_special_tokens({\"pad_token\": \"<pad>\", \"eos_token\": \"<eos>\", \"bos_token\": \"<bos>\"})\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "def generate_response(user_input, max_length=512):\n",
    "    # Tokenize the user input\n",
    "    inputs = tokenizer(\n",
    "        user_input,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=max_length\n",
    "    ).to('cuda')\n",
    "\n",
    "    # Generate response from the model\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_length=max_length,\n",
    "        num_beams=3,\n",
    "        early_stopping=True,\n",
    "        no_repeat_ngram_size=2\n",
    "    )\n",
    "\n",
    "    # Decode the response\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return response\n",
    "\n",
    "en_path = \"/data/uijih/forTendency_examples_ENKR.csv\"\n",
    "en_samples = pd.read_csv(en_path)\n",
    "\n",
    "shots_en_to_kr = \"\"\"\n",
    "Translate the following sentence into Korean\n",
    "This sentence contains the idiom '{Idiom}', which means '{Meaning}'. The Korean idiom similar to '{Idiom}' is '{KR_Idiom}', which means '{KR_Meaning}'.\n",
    "Translate the entire sentence by naturally replacing the English idiomatic expression with this Korean idiomatic expression. For a natural translation, feel free to change the tense or voice of the idiom if necessary.\n",
    "{}\n",
    "\"\"\"\n",
    "\n",
    "# Prompt creation function\n",
    "def create_prompt(english_sentence=None):\n",
    "    prompt = shots_en_to_kr.format(\n",
    "        Sentence=english_sentence['Sentence'],\n",
    "        Idiom=english_sentence['Idiom'],\n",
    "        Meaning=english_sentence['Meaning'],\n",
    "        KR_Idiom=english_sentence['KR_Idiom'],\n",
    "        KR_Meaning=english_sentence['KR_Meaning']\n",
    "    )\n",
    "    return prompt\n",
    "\n",
    "# Initialize dataframe to store results\n",
    "results_en_to_kr_df = pd.DataFrame(columns=['Original_EN_Sentence', 'Label', 'EN_to_KR_Translation'])\n",
    "\n",
    "# Perform translation on samples and save results\n",
    "for idx, (_, item) in enumerate(en_samples.iterrows()):\n",
    "    # Create prompts\n",
    "    final_prompt_en_to_kr = create_prompt(item)\n",
    "\n",
    "    # Generate model responses\n",
    "    translation_response_en_to_kr = generate_response(final_prompt_en_to_kr)\n",
    "\n",
    "    # Extract the translation by removing the prompt\n",
    "    en_to_kr_translation = translation_response_en_to_kr[len(final_prompt_en_to_kr):].strip()\n",
    "    #en_to_kr_translation = translation_response_en_to_kr\n",
    "\n",
    "    # Add results to the dataframe (EN to KR)\n",
    "    new_row_en_to_kr = {\n",
    "        'Original_EN_Sentence': item['Sentence'],\n",
    "        'Label': item['KR_Idiom'],\n",
    "        'EN_to_KR_Translation': en_to_kr_translation\n",
    "    }\n",
    "    results_en_to_kr_df = pd.concat([results_en_to_kr_df, pd.DataFrame([new_row_en_to_kr])], ignore_index=True)\n",
    "\n",
    "    # Print results\n",
    "    print(f\"Source: {item['Sentence']}\")\n",
    "    print(f\"Assistant (EN to KR): {en_to_kr_translation}\")\n",
    "    print(\"-\" * 100)\n",
    "\n",
    "# Save results to CSV file\n",
    "output_csv_en_to_kr_path = 'ten_en_to_kr.csv'\n",
    "results_en_to_kr_df.to_csv(output_csv_en_to_kr_path, index=False)\n",
    "\n",
    "print(f\"EN to KR translation results successfully saved to {output_csv_en_to_kr_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3.11.3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
